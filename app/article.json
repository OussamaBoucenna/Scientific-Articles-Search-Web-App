{
  "title": "A Survey of Sequential Pattern Mining",
  "authors": [
    {
      "name": "Yun Sing Koh",
      "position": "Senior Lecturer",
      "affiliation": "The University of Auckland, New Zealand"
    },
    {
      "name": "Rincy Thomas",
      "position": "Associate Professor",
      "affiliation": "SCT, Bhopal, India"
    },
    {
      "name": "Philippe Fournier-Viger",
      "position": "Full Professor",
      "affiliation": "Harbin Institute of Technology Shenzhen Grad. School, China"
    },
    {
      "name": "Jerry Chun-Wei Lin",
      "position": "Associate Professor",
      "affiliation": "Harbin Institute of Technology, Shenzhen, China"
    },
    {
      "name": "Rage Uday Kiran",
      "position": "Specially Appointed Research Assistant Professor",
      "affiliation": "University of Tokyo, Japan"
    }
  ],
  "Institution": "{\n\"Yun Sing Koh\": \"Department of Computer Science, University of Auckland, Auckland, New Zealand\",\n\"Rincy Thomas\": \"Department of Computer Science and Engineering, SCT, Bhopal, India\",\n\"Jerry Chun-Wei Lin\": \"Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China\",\n\"Rage Uday Kiran\": \"University of Tokyo, Tokyo, Japan\",\n\"Philippe Fournier-Viger\": \"Harbin Institute of Technology Shenzhen Grad. School, China\"\n}",
  "Abstract": "Abstract. Discovering unexpected and useful patterns in databases is a fundamental data mining task. In recent years, a trend in data mining has been to design algorithms for discovering patterns in sequential data. One of the most popular data mining tasks on sequences is sequential pattern mining. It consists of discovering interesting subsequences in a set of sequences, where the interestingness of a subsequence can be measured in terms of various criteria such as its occurrence frequency, length, and pro\ufb01t. Sequential pattern mining has many real-life applications since data is encoded as sequences in many \ufb01elds such as bioinformatics, e-learning, market basket analysis, text analysis, and webpage click-stream analysis. This paper surveys recent studies on sequential pattern mining and its applications. The goal is to provide both an introduction to sequential pattern mining, and a survey of recent advances and research opportunities. The paper is divided into four main parts. First, the task of sequential pattern mining is de\ufb01ned and its applications are reviewed. Key concepts and terminology are introduced. Moreover, main approaches and strategies to solve sequential pattern mining problems are presented. Limitations of traditional sequential pattern mining approaches are also highlighted, and popular variations of the task of sequential pattern mining are presented. The paper also presents research opportunities and the relationship to other popular pattern mining problems. Lastly, the paper also discusses open-source implementations of sequential pattern mining algorithms. ",
  "Keywords": "",
  "Reference": "REFERENCES\n[1] R. Agrawal and R. Srikant, \u201cFast algorithms for mining association rules,\u201d The International Con-\nference on Very Large Databases, pp. 487\u2013499, 1994.\n[2] R. Agrawal, and R. Srikant, \u201cMining sequential patterns,\u201d The International Conference on Data\nEngineering, pp. 3\u201314, 1995.\n[3] J. Ayres, J. Flannick, J. Gehrke, and T. Yiu, \u201cSequential pattern mining using a bitmap represen-\ntation,\u201d ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.\n429\u2013435, 2002.\n[4] S. Aseervatham, A. Osmani, and E. Viennet, \u201cbitSPADE: A lattice-based sequential pattern mining\nalgorithm using bitmap representation,\u201d The International Conference on Data Mining, pp. 792\u2013797,\n2006.\n[5] C. F. Ahmed, S. K. Tanbeer, and B. S. Jeong, \u201cA novel approach for mining high-utility sequential\npatterns in sequence databases,\u201d Electronics and Telecommunications Research Institute journal, vol.\n32(5), pp. 676\u2013686, 2010.\n[6] C. C. Aggarwal, Data mining: the textbook, Heidelberg:Springer, 2015.\n[7] G. Aliberti, A. Colantonio, R. Di Pietro, and R. Mariani, \u201cEXPEDITE: EXPress closED ITemset\nenumeration,\u201d Expert Systems with Applications, vol. 42(8), pp. 3933\u20133944, 2015.\n[8] O. K. Alkan, and P. Karagoz, \u201cCRoM and HuspExt: improving e\ufb03ciency of high utility sequential\npattern extraction,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 27(10), pp. 2645\u2013\n2657, 2015.\n[9] A. Barron, J. Rissanen, and B. Yu, \u201cThe minimum description length principle in coding and mod-\neling,\u201d IEEE Transactions on Information Theory, vol. 44(6), pp.2743\u20132760, 1998.\n[10] F. Bonchi, and C. Lucchese, \u201cPushing tougher constraints in frequent pattern mining,\u201d The Paci\ufb01c-\nAsia Conference on Knowledge Discovery and Data Mining, pp. 114\u2013124, 2005.\n[11] M. A. Bhuiyanm, and M. Al Hasan, \u201cAn iterative MapReduce based frequent subgraph mining\nalgorithm,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 27(3), pp. 608\u2013620, 2015.\n[12] J. H. Chang, and W. S. Lee, \u201cFinding recent frequent itemsets adaptively over online data streams,\u201d\nACM SIGKDD International Conference Knowledge Discovery and Data Mining, pp. 487\u2013492, 2003.\n[13] H. Cheng, X. Yan, and J. Han, \u201cIncSpan: incremental mining of sequential patterns in large data-\nbase,\u201d ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.\n527\u2013532, 2004.\n[14] J. H. Chang, and W. S. Lee, \u201cE\ufb03cient mining method for retrieving sequential patterns over online\ndata streams,\u201d Journal of Information Science, vol. 31(5), pp. 420\u2013432, 2005.\n[15] L. Chang, T. Wang, D. Yang, and H. Luan, \u201cSeqstream: Mining closed sequential patterns over\nstream sliding windows,\u201d IEEE International Conference on Data Mining, pp. 83\u201392, 2008.\n[16] C. H. Chen, A. F. Li, and Y. C. Lee, \u201cActionable high-coherent-utility fuzzy itemset mining,\u201d Soft\nComputing, vol. 18(12), pp. 2413\u20132424, 2014.\nA Survey of Sequential Pattern Mining\n71\n[17] A. Camerra, T. Palpanas, J. Shieh, and E. Keogh, \u201ciSAX 2.0: Indexing and mining one billion time\nseries,\u201d IEEE International Conference on Data Mining, pp. 58\u201367, 2010.\n[18] J. H. Chang, \u201cMining weighted sequential patterns in a sequence database with a time-interval\nweight,\u201d Knowledge-Based Systems, vol. 24(1), pp. 1\u20139, 2011.\n[19] R. Campisano, F. Porto, E. Pacitti, F. Masseglia, and E. Ogasawara, \u201cSpatial sequential pattern\nmining for seismic data,\u201d The Brazilian Symposium on Databases, pp. 241\u2013246, 2016.\n[20] L. Cao, X. Dong, and Z. Zheng, \u201ce-NSP: E\ufb03cient negative sequential pattern mining,\u201d Arti\ufb01cial\nIntelligence, vol. 235, pp. 156\u2013182, 2016.\n[21] C. Fiot, A. Laurent, and M. Teisseire, \u201cFrom crispness to fuzziness: Three algorithms for soft\nsequential pattern mining,\u201d IEEE Transactions on Fuzzy Systems, vol. 15(6), pp. 1263\u20131277, 2007.\n[22] P. Fournier-Viger, R. Nkambou, and E. Mephu Nguifo, \u201cA Knowledge discovery framework for learn-\ning task models from user interactions in intelligent tutoring systems,\u201d The Mexican International\nConference on Arti\ufb01cial Intelligence, pp. 765\u2013778, 2008.\n[23] T. C. Fu, \u201cA review on time series data mining,\u201d Engineering Applications of Arti\ufb01cial Intelligence,\nvol. 24(1), pp. 164\u2013181, 2011.\n[24] P. Fournier-Viger, and V. S. Tseng, \u201cMining top-k sequential rules, The International Conference on\nAdvanced Data Mining and Applications, pp. 180\u2013194, 2011.\n[25] P. Fournier-Viger, T. Gueniche, and V. S. Tseng, \u201cUsing partially-ordered sequential rules to generate\nmore accurate sequence prediction, The International Conference on Advanced Data Mining and\nApplications, pp. 431\u2013442, 2012.\n[26] P. Fournier-Viger, C.-W. Wu, and V. S. Tseng, \u201cMining top-k association rules, The Canadian\nConference on Arti\ufb01cial Intelligence, pp.61\u201373, 2012.\n[27] P. Fournier-Viger, A. Gomariz, T. Gueniche, E. Mwamikazi, and R. Thomas, \u201cTKS: E\ufb03cient Min-\ning of Top-K Sequential Patterns,\u201d The International Conference on Advanced Data Mining and\nApplications, pp. 109\u2013120, 2013.\n[28] P. Fournier-Viger, C.-W. Wu, and V. S. Tseng, \u201cMining Maximal Sequential Patterns without\nCandidate Maintenance,\u201d The International Conference on Advanced Data Mining and Applications,\npp. 169\u2013180, 2013.\n[29] P. Fournier-Viger, A. Gomariz, T. Gueniche, E. Mwamikazi, and R. Thomas, \u201cTKS: E\ufb03cient mining\nof top-k sequential patterns,\u201d The International Conference on Advanced Data Mining and Applica-\ntions, pp. 109\u2013120, 2013.\n[30] P. Fournier-Viger, A. Gomariz, M. Campos, and R. Thomas, \u201cFast Vertical Mining of Sequential\nPatterns Using Co-occurrence Information,\u201d The Paci\ufb01c-Asia Conference on Knowledge Discovery\nand Data Mining, pp. 40\u201352, 2014.\n[31] P. Fournier-Viger, C.-W. Wu, A. Gomariz, and V. S. Tseng, \u201cVMSP: E\ufb03cient vertical mining of\nmaximal sequential patterns,\u201d The Canadian Conference on Arti\ufb01cial Intelligence, pp. 83\u201394, 2014.\n[32] P. Fournier-Viger, A. Gomariz, M. Sebek, M. Hlosta, \u201cVGEN: fast vertical mining of sequential\ngenerator patterns,\u201d The International Conference on Data Warehousing and Knowledge Discovery,\npp. 476\u2013488, 2014.\n[33] P. Fournier-Viger, C. W. Wu, V. S. Tseng, \u201cNovel concise representations of high utility itemsets\nusing generator patterns,\u201d The Internatioanl Conference on Advanced Data Mining and Applications,\npp. 30\u201343, 2014.\n[34] P. Fournier-Viger, T. Gueniche, S. Zida, and V. S. Tseng, \u201cERMiner: sequential rule mining using\nequivalence classes,\u201d The International Symposium on Intelligent Data Analysis, pp. 108\u2013119, 2014.\n[35] P. Fournier-Viger, A. Gomariz, T. Gueniche, A. Soltani, C.-W. Wu, V. S. Tseng, \u201cSPMF: A java\nopen-source pattern mining library,\u201d Journal of Machine Learning Research, vol. 15, pp. 3389\u20133393,\n2014.\n[36] P. Fournier-Viger, C. W. Wu, S. Zida, V. S. Tseng, \u201cFHM: Faster high-utility itemset mining using\nestimated utility co-occurrence pruning,\u201d The International Symptom on Methodologies for Intelli-\ngent Systems, pp. 83\u201392, 2014.\n[37] M. Fabregue, A. Braud, S. Bringay, F. Le Ber, and M. Teisseire, \u201cMining closed partially ordered\npatterns, a new optimized algorithm,\u201d Knowledge-Based Systems, vol. 79, pp. 68\u201379, 2015.\n[38] P. Fournier-Viger, C. W. Wu, V. S. Tseng, L. Cao, R. Nkambou, \u201cMining partially-ordered sequential\nrules common to multiple sequences,\u201d IEEE Transactions on Knowledge and Data Engineering, vol.\n27(8), pp. 2203\u20132216, 2015.\n[39] F. Fumarola, P. F. Lanotte, M. Ceci, and D. Malerba, \u201cCloFAST: closed sequential pattern mining\nusing sparse and vertical id-lists,\u201d Knowledge and Information Systems, vol. 48(2), pp. 1\u201335, 2015.\n72\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\n[40] P. Fournier-Viger, C. W. Lin, A. Gomariz, A. Soltani, Z. Deng, H. T. Lam, \u201cThe SPMF open-\nsource data mining library version 2,\u201d The European Conference on Principles of Data Mining and\nKnowledge Discovery, pp. 36-40, 2016.\n[41] P. Fournier-Viger, C. W. Lin, Q. H. Duong, and T. L. Dam, \u201cPHM: Mining periodic high-utility\nitemsets,\u201d The Industrial Conference on Data Mining, pp. 64\u201379, 2016.\n[42] M. N. Garofalakis, R. Rastogi, and K. Shim, \u201cSPIRIT: Sequential pattern mining with regular\nexpression constraints,\u201d The International Conference on Very Large Databases, pp. 223\u2013234, 1999.\n[43] B. Goethals, \u201cSurvey on frequent pattern mining,\u201d University of Helsinki, 2003.\n[44] E. Z. Guan, X. Y. Chang, Z. Wang, and C. G. Zhou, \u201cMining maximal sequential patterns,\u201d The\nInternational Conference on Neural Networks and Brain, pp. 525\u2013528, 2005.\n[45] R. A. Garcia-Hernandez, J. F. Martanez-Trinidad, and J. A. Carrasco-Ochoa, \u201cA new algorithm for\nfast discovery of maximal sequential patterns in a document collection,\u201d The International Confer-\nence on Intelligent Text Processing and Computational Linguistics, pp. 514\u2013523, 2006.\n[46] C. Gao, J. Wang, Y. He, and L. Zhou, \u201cE\ufb03cient mining of frequent sequence generators,\u201d The\nInternational Conference on the World Wide Web, pp. 1051\u20131052, 2008.\n[47] K. Gouda, M. Hassaan, and M. J. Zaki, \u201cPrism: An e\ufb00ective approach for frequent sequence mining\nvia prime-block encoding,\u201d Journal of Computer and System Sciences, vol. 76(1), pp. 88\u2013102, 2010.\n[48] A. Gomariz, M. Campos, R. Marin, and B. Goethals, \u201cClaSP: An e\ufb03cient algorithm for mining\nfrequent closed sequences,\u201d The Paci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining,\npp. 50\u201361, 2013.\n[49] J. Ge, Y. Xia, and J. Wang, \u201cTowards e\ufb03cient sequential pattern mining in temporal uncertain\ndatabases, The Paci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining, pp. 268\u2013279,\n2015.\n[50] J. Ge, Y. Xia, and J. Wang, \u201cMining uncertain sequential patterns in iterative mapReduce,\u201d The\nPaci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining, pp. 243\u2013254, 2015.\n[51] J. Han, J. Pei, B. Mortazavi-Asl, Q. Chen, U. Dayal, and M. C. Hsu, \u201cFreeSpan: frequent pattern-\nprojected sequential pattern mining,\u201d ACM SIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining, pp. 355\u2013359, 2000.\n[52] T. P. Hong, K. Y. Lin, and S. L. Wang, \u201cMining fuzzy sequential patterns from multiple-items\ntransactions,\u201d The IFSA World Congress and the NAFIPS International Conference, pp. 1317\u20131321,\n2001.\n[53] J. Han, J. Pei, Y. Ying, and R. Mao, \u201cMining frequent patterns without candidate generation: a\nfrequent-pattern tree approach,\u201d Data Mining and Knowledge Discovery, vol. 8(1), pp. 53\u201387, 2004.\n[54] J. Ho, L. Lukov, and S. Chawla, \u201cSequential pattern mining with constraints on large protein\ndatabases,\u201d The International Conference on Management of Data, pp. 89\u2013100, 2005.\n[55] C. C. Ho, H. F\u2019. Li, F\u2019. F. Kuo,and S. Y. Lee, \u201cIncremental mining of sequential patterns over a\nstream sliding window,\u201d IEEE International Conference on Data Mining Workshops, pp. 677\u2013681,\n2006.\n[56] K. Y. Huang, C. H. Chang, J. H. Tung, and C. T. Ho, \u201cCOBRA: closed sequential pattern min-\ning using bi-phase reduction approach,\u201d The International Conference on Data Warehousing and\nKnowledge Discovery, pp. 280\u2013291, 2006.\n[57] S. C. Hsueh, M. Y. Lin, and C. L. Chen, \u201cMining negative sequential patterns for e-commerce\nrecommendations,\u201d IEEE Asia-Paci\ufb01c Services Computing Conference, pp. 1213\u20131218, 2008.\n[58] J. Han, J. Pei, and M. Kamber, Data mining: concepts and techniques, Amsterdam:Elsevier, 2011.\n[59] C. Jiang, F. Coenen, and M. Zito, \u201cA survey of frequent subgraph mining algorithms,\u201d The Knowl-\nedge Engineering Review, vol. 28(1), pp. 75\u2013105, 2013.\n[60] R. U. Kiran, and P. K. Reddy, \u201cMining rare periodic-frequent patterns using multiple minimum\nsupports,\u201d The International Conference on Management of Data, 2009.\n[61] R. U. Kiran, M. Kitsuregawa, and P. K. Reddy, \u201cE\ufb03cient discovery of periodic-frequent patterns in\nvery large databases,\u201d Journal of Systems and Software, vol. 112, pp. 110\u2013121, 2016.\n[62] M. Y. Lin, and S. Y. Lee, \u201cImproving the e\ufb03ciency of interactive sequential pattern mining by\nincremental pattern discovery,\u201d The International Conference on System Sciences, pp. 68\u201376, 2002.\n[63] S. Lu, and C. Li, \u201cAprioriAdjust: An e\ufb03cient algorithm for discovering the maximum sequential\npatterns,\u201d The International Workshop on Knowledge Grid and Grid Intelligence, 2004.\n[64] C. Luo, and S. Chung, \u201cE\ufb03cient mining of maximal sequential patterns using multiple samples,\u201d\nSIAM International Conference on Data Mining, pp. 415\u2013426, 2005.\nA Survey of Sequential Pattern Mining\n73\n[65] C. Lucchese, S. Orlando, and R. Perego, \u201cFast and memory e\ufb03cient mining of frequent closed\nitemsets,\u201d IEEE Transactions on Knowledge Discovery and Data Engineering, vol. 18(1), pp. 21\u201336,\n2006.\n[66] J. Lin, E. Keogh, L. Wei, and S. Lonardi, \u201cExperiencing SAX: a novel symbolic representation of\ntime series,\u201d Data Mining and Knowledge Discovery, vol. 15(2), pp. 107\u2013144, 2007.\n[67] N. P. Lin, W.-H. Hao, H.-J. Chen, H.-E. Chueh, and C.-I. Chang, \u201cFast mining maximal sequential\npatterns,\u201d The International Conference on Simulation, Modeling and Optimization, pp. 405-408,\n2007.\n[68] P. Lenca, B. Vaillant, P. Meyer, and S. Lallich, \u201cAssociation rule interestingness measures: Ex-\nperimental and theoretical studies,\u201d The Quality Measures in Data Mining Workshop, pp. 51\u201376,\n2007.\n[69] Y. S. Lee, and S. J. Yen, \u201cIncremental and interactive mining of web traversal patterns,\u201d Information\nSciences, vol. 178(2), pp. 287\u2013306, 2008.\n[70] D. Lo, S. C. Khoo, and J. Li, \u201cMining and ranking generators of sequential patterns,\u201d SIAM Inter-\nnational Conference on Data Mining, pp. 553\u2013564, 2008.\n[71] M. Liu, J. Qu, \u201cMining high utility itemsets without candidate generation,\u201d ACM International Con-\nference Information and Knowledge Management, pp.55\u201364, 2012.\n[72] G. C. Lan, T. P. Hong, V. S. Tseng, and S. L. Wang, \u201cApplying the maximum utility measure in\nhigh utility sequential pattern mining,\u201d Expert Systems with Applications, vol. 41(11), pp. 5071\u20135081,\n2014.\n[73] J. C. W. Lin, L. Tin, P. Fournier-Viger, and T. P. Hong, \u201cA fast Algorithm for mining fuzzy frequent\nitemsets,\u201d Journal of Intelligent and Fuzzy Systems, vol. 9(6), pp.2373\u20132379, 2015.\n[74] J. C. W. Lin, T. P. Hong, W. Gan, H. Y. Chen, and S. T. Li, \u201cIncrementally updating the discovered\nsequential patterns based on pre-large concept,\u201d Intelligent Data Analysis, vol. 19(5), pp. 1071\u20131089,\n2015.\n[75] J. C. W. Lin, T. P. Hong, and G. C. Lan, \u201cUpdating the sequential patterns in dynamic databases\nfor customer sequences deletion,\u201d Journal of Internet Technology, vol. 16(3), pp. 369\u2013377, 2015.\n[76] J. C. W. Lin, W. Gan, P. Fournier-Viger, and T. P. Hong, \u201cMaintenance sequential patterns for\nsequence modi\ufb01cation in dynamic databases,\u201d International Journal of Software Engineering and\nKnowledge Engineering, vol. 26(8), pp. 1285\u20131313, 2016.\n[77] H. Mannila, H. Toivonen, A. I. Verkamo, \u201cDiscovery of frequent episodes in event sequences,\u201d Data\nMining and Knowledge Discovery, vol. 1(3), pp. 259\u2013289, 1997.\n[78] F. Masseglia, P. Poncelet, and M. Teisseire, \u201cIncremental mining of sequential patterns in large\ndatabases,\u201d Data and Knowledge Engineering, vol. 46(1), pp. 97\u2013121, 2003.\n[79] N. R. Mabroukeh, and C. I. Ezeife, \u201cA taxonomy of sequential pattern mining algorithms,\u201d ACM\nComputing Surveys, vol. 43(1), 2010.\n[80] M. Muzammal, and R. Raman, \u201cOn probabilistic models for uncertain sequential pattern mining,\u201d\nThe International Conference on Advanced Data Mining and Applications, pp. 60\u201372, 2010.\n[81] M. Muzammal, and R. Raman, \u201cMining sequential patterns from probabilistic databases,\u201d Knowl-\nedge and Information Systems, vol. 44(2), pp. 325\u2013358, 2015.\n[82] S. N. Nguyen, X. Sun, M. E. Orlowska, \u201cImprovements of IncSpan: Incremental mining of sequential\npatterns in large database,\u201d The Paci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining,\npp. 442\u2013451, 2005.\n[83] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, \u201cDiscovering frequent closed itemsets for associ-\nation rules,\u201d The International Conference on Database Theory, pp. 398\u2013416, 1999.\n[84] S. Parthasarathy, M. J. Zaki, M. Ogihara, and S. Dwarkadas, \u201cIncremental and interactive sequence\nmining,\u201d The International Conference on Information and Knowledge Management, pp. 251\u2013258,\n1999.\n[85] H. Pinto, J. Han, J. Pei, K. Wang, Q. Chen, and U. Dayal, \u201cMulti-dimensional sequential pattern\nmining,\u201d The International Conference on Information and Knowledge Management, pp. 81\u201388,\n2001.\n[86] J. Pei, J. Han, H. Lu, S. Nishio, S. Tang, and D. Yang, \u201cH-mine: Hyper-structure mining of frequent\npatterns in large databases,\u201d IEEE International Conference on Data Mining, pp. 441\u2013448, 2001.\n[87] J. Pei, J. Han, and L. V. Lakshmanan, \u201cMining frequent itemsets with convertible constraints,\u201d The\nInternational Conference on Data Engineering, pp.433\u2013442, 2001.\n[88] J. Pei, J. Han, and L. V. Lakshmanan, \u201cPushing convertible constraints in frequent itemset mining,\u201d\nData Mining and Knowledge Discovery, vol. 8(3), pp. 227\u2013252, 2004.\n74\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\n[89] J. Pei, J. Han, B. Mortazavi-Asl, J. Wang, H. Pinto, Q. Chen, U. Dayal, and M. C. Hsu, \u201cMining\nsequential patterns by pattern-growth: The pre\ufb01xspan approach,\u201d IEEE Transactions on knowledge\nand data engineering, vol. 16(11), pp. 1424\u20131440, 2004.\n[90] J. Pei, H. Wang, J. Liu, K. Wang, J. Wang, and P. S. Yu, \u201cDiscovering frequent closed partial orders\nfrom strings,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 18(11), pp. 1467\u20131481,\n2006.\n[91] J. Pei, J. Han, and W. Wang, \u201cConstraint-based sequential pattern mining: the pattern-growth\nmethods,\u201d Journal of Intelligent Information Systems, vol. 28(2), pp. 133\u201360, 2007.\n[92] T. T. Pham, J. Luo, T.-P. Hong, and B. Vo, \u201cMSGPs: a novel algorithm for mining sequential\ngenerator patterns,\u201d The International Conference on Computational Collective Intelligence, pp.\n393\u2013401, 2012.\n[93] Y. W. T. Pramono, \u201cAnomaly-based Intrusion Detection and Prevention System on Website Us-\nage using Rule-Growth Sequential Pattern Analysis,\u201d The International Conference on Advanced\nInformatics, Concept Theory and Applications, pp. 203\u2013208, 2014.\n[94] J. M. Pokou, P. Fournier-Viger, and C. Moghrabi, \u201cAuthorship attribution using small sets of fre-\nquent part-of-speech skip-grams,\u201d The International Florida Arti\ufb01cial Intelligence Research Society\nConference, pp. 86\u201391, 2016.\n[95] M. N. Quang, T. Dinh, U. Huynh, and B. Le, \u201cMHHUSP: An integrated algorithm for mining and\nhiding high utility sequential patterns,\u201d The International Conference on Knowledge and Systems\nEngineering, pp. 13\u201318, 2016.\n[96] C. Raissi, P. Poncelet, and M. Teisseire, \u201cSPEED: mining maxirnal sequential patterns over data\nstreams,\u201d IEEE Conference Intelligent Systems, pp. 546\u2013552, 2006.\n[97] J. D. Ren, J. Yang, and Y. Li, \u201cMining weighted closed sequential patterns in large databases,\u201d The\nInternational Conference on Fuzzy Systems and Knowledge Discovery, pp. 640\u2013644, 2008.\n[98] R. Srikant, and R. Agrawal, \u201cMining sequential patterns: Generalizations and performance improve-\nments,\u201d The International Conference on Extending Database Technology, pp. 1\u201317, 1996.\n[99] P. Songram, and V. Boonjing, \u201cClosed multidimensional sequential pattern mining,\u201d International\nJournal of Knowledge Management Studies, vol. 2(4), pp. 460\u2013479, 2008.\n[100] E. Salvemini, F. Fumarola, D. Malerba, and J. Han, \u201cFast sequence mining based on sparse id-lists,\u201d\nThe International Symposium on Methodologies for Intelligent Systems, pp. 316\u2013325, 2011.\n[101] A. Soulet, F. Rioult, \u201cE\ufb03ciently depth-\ufb01rst minimal pattern mining,\u201d The Paci\ufb01c-Asia Conference\non Knowledge Discovery and Data Mining, pp. 28\u201339, 2014.\n[102] S. J. Shin, D. S. Lee, and W. S. Lee, \u201cCP-tree: An adaptive synopsis structure for compressing\nfrequent itemsets over online data streams,\u201d Information Sciences, vol. 278(10), pp. 559-576, 2014.\n[103] L. Szathmary, P. Valtchev, A. Napoli, R. Godin, A. Boc, and V. Makarenkov. \u201cA fast compound\nalgorithm for mining generators, closed itemsets, and computing links between equivalence classes,\u201d\nAnnals of Mathematics and Arti\ufb01cial Intelligence, vol. 70(1-2), pp. 81\u2013105, 2014.\n[104] D. Schweizer, M. Zehnder, H. Wache, H. F. Witschel, D. Zanatta, and M. Rodriguez, \u201cUsing\nconsumer behavior data to reduce energy consumption in smart homes: Applying machine learning\nto save energy without lowering comfort of inhabitants,\u201d IEEE International Conference on Machine\nLearning and Applications, pp. 1123\u20131129, 2015.\n[105] S. K. Tanbeer, C. F. Ahmed, B. S. Jeong, and Y. K. Lee, \u201cDiscovering periodic-frequent patterns in\ntransactional databases,\u201d The Paci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining,\npp. 242\u2013253, 2009.\n[106] T. Uno, M. Kiyomi, and H. Arimura, \u201cLCM ver. 2:\nE\ufb03cient mining algorithms for fre-\nquent/closed/maximal itemsets,\u201d IEEE International Conference on Data Mining Workshop on\nFrequent Itemset Mining Implementations, 2004.\n[107] B. Vo, T. P. Hong, and B. Le, \u201cDBV-Miner: A Dynamic Bit-Vector approach for fast mining\nfrequent closed itemsets,\u201d Expert Systems with Applications, vol. 39(8), pp. 7196\u2013206, 2012.\n[108] J. Wang, J. Han, and C. Li, \u201cFrequent closed sequence mining without candidate maintenance,\u201d\nIEEE Transactions on Knowledge Data Engineering, vol. 19(8), pp. 1042\u20131056, 2007.\n[109] Y. Xifeng, H. Jiawei, and R. Afshar, \u201cCloSpan: Mining Closed Sequential Patterns in Large Data\nBase,\u201d SIAM International Conference on Data Mining, pp. 166\u2013177, 2003.\n[110] X. Yan, and J. Han, \u201cgSpan: Graph-based substructure pattern mining,\u201d IEEE International Con-\nference Data Mining, pp.721\u2013724, 2002.\n[111] Z. Yang, and M. Kitsuregawa, \u201cLAPIN-SPAM: An improved algorithm for mining sequential pat-\ntern,\u201d The International Conference on Data Engineering Workshops, pp. 1222\u20131222, 2005.\nA Survey of Sequential Pattern Mining\n75\n[112] U. Yun, and J. J. Leggett, \u201cWSpan:\nWeighted Sequential pattern mining in large sequence\ndatabases,\u201d The International IEEE Conference Intelligent Systems, pp. 512\u2013517, 2006.\n[113] H. Yu, H. Yamana, \u201cGeneralized sequential pattern mining with item intervals,\u201d Journal of Com-\nputers, vol. 1(3), pp. 51\u201360, 2006.\n[114] S. Yi, T. Zhao, Y. Zhang, S. Ma, and Z. Che, \u201cAn e\ufb00ective algorithm for mining sequential gener-\nators,\u201d Procedia Engineering, vol. 15, pp. 3653\u20133657, 2011.\n[115] J. Yin, Z. Zheng, and L. Cao, \u201cUSpan: an e\ufb03cient algorithm for mining high utility sequential\npatterns,\u201d ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.\n660\u2013668, 2012.\n[116] M. J. Zaki, \u201cScalable algorithms for association mining,\u201d IEEE Transactions on Knowledge and\nData Engineering, vol. 12(3), pp. 372\u2013390, 2000.\n[117] M. J. Zaki, \u201cSPADE: An e\ufb03cient algorithm for mining frequent sequences,\u201d Machine learning, vol.\n42(1-2), pp. 31\u201360, 2001.\n[118] Z. Zheng, Y. Zhao, Z. Zuo, and L. Cao, \u201cNegative-GSP: an e\ufb03cient method for mining negative\nsequential patterns,\u201d The Australasian Data Mining Conference, pp. 63\u201367, 2009.\n[119] Z. Zheng, Y. Zhao, Z. Zuo, and L. Cao, \u201cAn e\ufb03cient GA-based algorithm for mining negative\nsequential patterns,\u201d The Paci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining, pp.\n262\u2013273, 2010.\n[120] M. J. Zaki, and C. J. Hsiao, \u201cCHARM: An e\ufb03cient algorithm for closed itemset mining,\u201d SIAM\nInternational Conference on Data Mining, pp.457\u2013473, 2012.\n[121] Z. Zhao, D. Yan, and W. Ng, \u201cMining probabilistically frequent sequential patterns in large uncer-\ntain databases,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 26(5), pp. 1171\u20131184,\n2014.\n[122] A. Zimmermann, \u201cUnderstanding episode mining techniques: Benchmarking on diverse, realistic,\narti\ufb01cial data,\u201d Intelligent Data Analysis, vol. 18(5), pp. 761\u2013791, 2014.\n[123] J. Zhang, Y. Wang, and D. Yang, \u201cCCSpan:\nMining closed contiguous sequential patterns,\u201d\nKnowledge-Based Systems, vol. 89, pp.1\u201313, 2015.\n[124] S. Ziebarth, I. A. Chounta, and H. U. Hoppe, \u201cResource access patterns in exam preparation\nactivities,\u201d The European Conference on Technology Enhanced Learning, pp. 497\u2013502, 2015.\n[125] S. Zida, P. Fournier-Viger, C. W. Wu, J. C. Lin, and V. S. Tseng, \u201cE\ufb03cient mining of high-utility\nsequential rules,\u201d The International Conference on Machine Learning and Data Mining, pp.157\u2013171,\n2015.\n[126] S. Zida, P. Fournier-Viger, J. C. W. Lin, C. W. Wu, and V. S. Tseng, \u201cEFIM: A highly e\ufb03cient algo-\nrithm for high-utility itemset mining,\u201d The Mexican International Conference Arti\ufb01cial Intelligence,\npp. 530\u2013546, 2015.\n76\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\nPhilippe Fournier-Viger (Ph.D.) is a full professor and Youth 1000\nscholar at the Harbin Institute of Technology Shenzhen Grad. School,\nChina. He received a Ph.D. in Computer Science at the University of\nQuebec in Montreal (2010). He has published more than 130 research\npapers in refereed international conferences and journals, which have\nreceived more than 1,100 citations. His research interests include data\nmining, pattern mining, sequence analysis and prediction, text mining,\ne-learning, and social network mining. He is the founder of the popular\nSPMF open-source data mining library, which has been cited in more\nthan 400 research papers since 2010. He is also editor-in-chief of the\nData Mining and Pattern Recognition journal.\nJerry Chun-Wei Lin received his Ph.D. in Computer Science and In-\nformation Engineering at the National Cheng Kung University, Tainan,\nTaiwan in 2010. He is now working as the associate professor at the\nHarbin Institute of Technology, Shenzhen, China. He has published\nmore than 200 research papers in refereed international conferences\nand journals, which have received more than 1000 citations. His re-\nsearch interests include data mining, privacy-preserving and security,\nbig data analytics, and social network. He is the co-leader of the popu-\nlar SPMF open-source data mining library and also the editor-in-chief\nof the Data Mining and Pattern Recognition (DSPR) journal.\nRage Uday Kiran (Ph.D.) is working as a Specially Appointed Re-\nsearch Assistant Professor at the University of Tokyo, Tokyo, Japan.\nHe recieved his PhD degree in computer science from International\nInstitute of Information Technology, Hyderabad, Telangana, India.\nHis current research interests include data mining, ICTs for agricul-\nture and recommender systems. He has published papers in Interna-\ntional Conference on Extending Database Technology (EDBT), The\nPaci\ufb01c-Asia Conference on Knowledge Discovery and Data Mining\n(PAKDD), Database Systems for Advanced Applications (DASFAA),\nInternational Conference on Database and Expert Systems Applica-\ntions (DEXA), International Conference on Scienti\ufb01c and Statistical\nDatabase Management (SSDBM), International Journal of Computa-\ntional Science and Engineering (IJCSE), Journal of Intelligent Information Systems (JIIS), and\nJournal of Systems and Software (JSS).\nA Survey of Sequential Pattern Mining\n77\nYun Sing Koh (Ph.D.) is currently a senior lecturer at the Depart-\nment of Computer Science, The University of Auckland, New Zealand.\nShe received her Ph.D. at the Department of Computer Science, Uni-\nversity of Otago, New Zealand. Her research interest is in the area of\ndata mining and machine learning, speci\ufb01cally data stream mining and\npattern mining.\nRincy Thomas (M. Tech)(M. Tech) is an associate professor at SCT,\nBhopal (M.P) India. He received his M.Tech from the Sagar Institute\nof Research & Technology Rajiv Gandhi Proudyogiki Vishwavidyalaya,\nBhopal, India.\n",
  "Text_integral": "introduction to sequential\npattern mining, and a survey of recent advances and research opportunities. The paper\nis divided into four main parts. First, the task of sequential pattern mining is de\ufb01ned and\nits applications are reviewed. Key concepts and terminology are introduced. Moreover,\nmain approaches and strategies to solve sequential pattern mining problems are presented.\nLimitations of traditional sequential pattern mining approaches are also highlighted, and\npopular variations of the task of sequential pattern mining are presented.\nThe paper\nalso presents research opportunities and the relationship to other popular pattern mining\nproblems.\nLastly, the paper also discusses open-source implementations of sequential\npattern mining algorithms.\nKeywords: Sequential pattern mining, Sequences, Frequent pattern mining, Itemset\nmining, Data Mining,\n54\nA Survey of Sequential Pattern Mining\n55\n1. Introduction. Data mining consists of extracting information from data stored in databases to un-\nderstand the data and/or take decisions. Some of the most fundamental data mining tasks are clustering,\nclassi\ufb01cation, outlier analysis, and pattern mining [6, 58]. Pattern mining consists of discovering interest-\ning, useful, and unexpected patterns in databases. This \ufb01eld of research has emerged in the 1990s with\nthe seminal paper of Agrawal and Srikant [1]. That paper introduced the Apriori algorithm, designed for\n\ufb01nding frequent itemsets, that is groups of items (symbols) frequently appearing together in a database\nof customer transactions. For example, the Apriori algorithm can be used to discover patterns such as\n{carrot juice, salad, kiwi} in a retail store database, indicating that these products are frequently bought\ntogether by customers.\nThe interest in pattern mining techniques comes from their ability to discover patterns that can be\nhidden in large databases and that are interpretable by humans, and hence useful for understanding\nthe data and for decision-making.\nFor example, a pattern {milk, chocolate cookies} can be used to\nunderstand customer behavior and take strategic decisions to increase sales such as co-promoting products\nand o\ufb00ering discounts.\nAlthough pattern mining has become very popular due to its applications in many domains, several\npattern mining techniques such as those for frequent itemset mining [1, 53, 116, 86, 106] and association\nrule mining [1] are aimed at analyzing data, where the sequential ordering of events is not taken into\naccount. Thus, if such pattern mining techniques are applied on data with time or sequential ordering\ninformation, this information will be ignored. This may result in the failure to discover important patterns\nin the data, or \ufb01nding patterns that may not be useful because they ignore the sequential relationship\nbetween events or elements. In many domains, the ordering of events or elements is important. For\nexample, to analyze texts, it is often relevant to consider the order of words in sentences [94]. In network\nintrusion detection, the order of events is also important [93].\nTo address this issue, the task of sequential pattern mining was proposed. It is a prominent solution\nfor analyzing sequential data [2, 98, 117, 4, 51, 89, 3, 47, 30, 111, 31, 32, 27, 28, 22, 100, 79].\nIt\nconsists of discovering interesting subsequences in a set of sequences, where the interestingness of a\nsubsequence can be measured in terms of various criteria such as its occurrence frequency, length, and\npro\ufb01t. Sequential pattern mining has numerous real-life applications due to the fact that data is naturally\nencoded as sequences of symbols in many \ufb01elds such as bioinformatics [108], e-learning [22], market basket\nanalysis [98], text analysis [94], energy reduction in smarthomes [104], webpage click-stream analysis [25]\nand e-learning [124]. Moreover, sequential pattern mining can also be applied to time series (e.g. stock\ndata), when discretization is performed as a pre-processing step [66]\nSequential pattern mining is a very active research topic, where hundreds of papers present new\nalgorithms and applications each year, including numerous extensions of sequential pattern mining for\nspeci\ufb01c needs. Because of this, it can be di\ufb03cult for newcomers to this \ufb01eld to get an overview of the\n\ufb01eld. To address this issue, a survey has been published in 2010 [79]. However, this survey is no longer\nup-to-date as it does not discuss the most recent techniques, advances and challenges in the \ufb01eld. In this\npaper, we aim to address this issue by presenting an up-to-date survey of sequential pattern mining that\ncan serve both as an introduction and as a guide to recent advances and opportunities in the \ufb01eld. The\nrest of this paper is organized as follows. The next section describes the problem of sequential pattern\nmining, and the main techniques employed in sequential pattern mining.\nThen, the paper discusses\npopular extensions of the problem of sequential pattern mining, and other problems in data mining that\nare closely related to sequential pattern mining. Then, the paper discusses research opportunities and\nopen-source implementations of sequential pattern mining algorithms. Finally, a conclusion is drawn.\n2. Sequential Pattern Mining. Two types of sequential data are commonly used in data mining [58]:\ntime-series and sequences. A time-series is an ordered list of numbers, while a sequence is an ordered\nlist of nominal values (symbols). For example, Fig. 1 (left) shows a time-series representing amounts of\nmoney, while Fig. 1 (right) depicts a sequence of symbols (letters). Both time-series and sequences are\nused in many domains. For instance, time-series are often used to represent data such as stock prices,\ntemperature readings, and electricity consumption readings, while sequences are used to represent data\nsuch as sentences in texts (sequences of words), sequences of items purchased by customers in retail stores,\nand sequences of webpages visited by users.\nThe problem of sequential pattern mining was proposed by Agrawal and Srikant [98], as the problem\nof mining interesting subsequences in a set of sequences.\nAlthough, it was originally designed to be\napplied to sequences, it can also be applied to time series after converting time-series to sequences using\ndiscretization techniques.\nFor example, Fig. 1 (right) shows a sequence representing the time-series\nshown in Fig. 1 (left), where the symbols a, b, c, d are de\ufb01ned as an increase of 10$, a decrease of 10$, an\n56\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\nincrease of 20$, and a decrease of 20$, respectively. There exists several ways of transforming time-series\nto sequences. Some of the most popular techniques are the SAX [66] and iSAX [17] algorithms. For more\ndetails about time-series transformations, the reader may refer to a survey of methods for time-series\ndata mining [23].\nA time-series\nA sequence\na, b , a, b , c, a, b, d \n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n2016-01-01\n2016-02-01\n2016-03-01\n2016-04-01\n2016-05-01\n2016-06-01\n2016-07-01\n2016-08-01\n2016-09-01\nProfit ($) \nFigure 1. A time-series (left) and a sequence (right)\nIn this paper, we are interested by sequences, as it is the type of data used in sequential pattern mining.\nDe\ufb01nitions related to sequences are given next with some illustrative examples. Let there be a set of\nitems (symbols) I = {i1, i2, . . . im}. An itemset X is a set of items such that X \u2286 I. Let the notation\n|X| denote the set cardinality or, in other words, the number of items in an itemset X. An itemset X\nis said to be of length k or a k-itemset if it contains k items (|X| = k). For example, consider that the\nset of symbols I = {a, b, c, d, e, f, g} represents the products sold in a retail store. The set {a, b, c} is an\nitemset containing three items, which may represent the purchases made by a customer on a given day.\nWithout loss of generality, assume that there exists a total order on items \u227a such as the lexicographical\norder (e.g. a \u227a b \u227a c \u227a d \u227a e \u227a f \u227a g). A sequence is an ordered list of itemsets s = \u27e8I1, I2, ..., In \u27e9 such\nthat Ik \u2286 I (1 \u2264 k \u2264 n). For example, consider the sequence \u27e8{a, b}, {c}, {f, g}, {g}, {e}\u27e9 representing\n\ufb01ve transactions made by a customer at a retail store. Each single letter represents an item. Items\nbetween curly brackets represent an itemset. This sequence indicates that a customer purchased items\na and b at the same time, then bought item c, then purchased items f and g at the same time, then\npurchased g, and \ufb01nally purchased e.\nA sequence sa = \u27e8A1, A2, . . . , An\u27e9 is said to be of length k or a k-sequence if it contains k items, or in\nother words if k = |A1| + |A2| + \u00b7 \u00b7 \u00b7 + |An|. For example, the sequence \u27e8{a, b}, {c}, {f, g}, {g}, {e}\u27e9 is a\n7-sequence.\nA sequence database SDB is a list of sequences SDB = \u27e8s1, s2, ..., sp\u27e9 having sequence identi\ufb01ers\n(SIDs) 1, 2...p. For instance, a sequence database is shown in Table 1, which contains four sequences\nhaving the SIDs 1, 2, 3 and 4. These sequences could, for example, represent purchases made by four\ncustomers.\nTable 1. A sequence database\nSID\nSequence\n1\n\u27e8{a, b}, {c}, {f, g}, {g}, {e}\u27e9\n2\n\u27e8{a, d}, {c}, {b}, {a, b, e, f}\u27e9\n3\n\u27e8{a}, {b}, {f}, {e}\u27e9\n4\n\u27e8{b}, {f, g}\u27e9\nA sequence sa = \u27e8A1, A2, ..., An\u27e9 is said to be contained in another sequence sb = \u27e8B1, B2, ..., Bm\u27e9 if and\nonly if there exist integers 1 \u2264 i1 < i2 < ... < in \u2264 m such that A1 \u2286 Bi1, A2 \u2286 Bi2, ..., An \u2286 Bin (denoted\nas sa \u2291 sb). For example, the sequence \u27e8{b}, {f, g}\u27e9 is contained in sequence \u27e8{a, b}, {c}, {f, g}, {g}, {e}\u27e9,\nwhile the sequence \u27e8{b}, {g}, {f}\u27e9 is not. If a sequence sa is contained in a sequence sb, sa is said to be\na subsequence of sb.\nThe goal of sequential pattern mining is to discover interesting subsequences in a sequence database,\nthat is sequential relationships between items that are interesting for the user. Various measures can be\n, g},{e}\nA Survey of Sequential Pattern Mining\n57\nTable 2. Sequential patterns found in the database of Table. 1\nPattern\nSup.\nClosed?\nMaximal?\nGenerator?\n\u27e8{a}\u27e9\n3\nyes\nno\nno\n\u27e8{a}, {g}\u27e9\n2\nno\nno\nyes\n\u27e8{a}, {g}, {e}\u27e9\n2\nyes\nyes\nno\n\u27e8{a}, {f}\u27e9\n3\nyes\nno\nno\n\u27e8{a}, {f}, {e}\u27e9\n2\nyes\nyes\nno\n\u27e8{a}, {c}\u27e9\n2\nno\nno\nyes\n\u27e8{a}, {c}, {f}\u27e9\n2\nyes\nyes\nno\n\u27e8{a}, {c}, {e}\u27e9\n2\nyes\nyes\nno\n\u27e8{a}, {b}\u27e9\n2\nno\nno\nyes\n\u27e8{a}, {b}, {f}\u27e9\n2\nyes\nyes\nno\n\u27e8{a}, {b}, {e}\u27e9\n2\nyes\nyes\nno\n\u27e8{a}, {e}\u27e9\n3\nyes\nno\nyes\n\u27e8{a, b}\u27e9\n2\nyes\nyes\nno\n\u27e8{b}\u27e9\n4\nno\nno\nyes\n\u27e8{b}, {g}\u27e9\n3\nyes\nno\nno\n\u27e8{b}, {g}, {e}\u27e9\n2\nyes\nyes\nno\n\u27e8{b}, {f}\u27e9\n4\nyes\nno\nno\n\u27e8{b}, {f, g}\u27e9\n2\nyes\nyes\nno\n\u27e8{b}, {f}, {e}\u27e9\n2\nyes\nyes\nno\n\u27e8{b}, {e}\u27e9\n3\nyes\nno\nno\n\u27e8{c}\u27e9\n2\nno\nno\nyes\n\u27e8{c}, {f}\u27e9\n2\nno\nno\nyes\n\u27e8{c}, {e}\u27e9\n2\nno\nno\nyes\n\u27e8{e}\u27e9\n3\nno\nno\nyes\n\u27e8{f}\u27e9\n4\nno\nno\nyes\n\u27e8{f, g}\u27e9\n2\nno\nno\nyes\n\u27e8{f}, {e}\u27e9\n2\nno\nno\nyes\n\u27e8{g}\u27e9\n3\nno\nno\nyes\n\u27e8{g}, {e}\u27e9\n2\nno\nno\nyes\nused to assess how interesting a subsequence is. In the original problem of sequential pattern mining,\nthe support measure is used. The support (or absolute support) of a sequence sa in a sequence database\nSDB is de\ufb01ned as the number of sequences that contain sa, and is denoted by sup(sa). In other words,\nsup(sa) = |{s|s \u2291 sa \u2227s \u2208 SDB}|. For example, the support of the sequence \u27e8{b}, {f, g}\u27e9 in the database\nof Table 1 is 2 because this sequence appears in two sequences (Sequence 1 and Sequence 4). Note that\nsome papers de\ufb01ne the support of a sequence X as a ratio. This de\ufb01nition called the relative support is\nrelSup(sa) = sup(sa)/|SDB|, that is the number of sequences containing sa divided by the number of\nsequences in the database. For example, the relative support of the itemset \u27e8{b}, {f, g}\u27e9 is 0.5.\nSequential pattern mining is the task of \ufb01nding all frequent subsequences in a sequence database. A\nsequence s is said to be a frequent sequence or a sequential pattern if and only if sup(s) \u2265 minsup, for\na threshold minsup set by the user [98]. The assumption is that frequent subsequences are interesting\nto the user. For instance, consider the database of Table 1, and assume that the user sets minsup = 2\nto \ufb01nd all subsequences appearing in at least two sequences. Table 2 shows the 29 sequential patterns\nfound in the database for minsup = 2, and their support values. For example, the patterns \u27e8{a}\u27e9 and\n\u27e8{a}, {g}\u27e9 are frequent and have a support of 3 and 2 sequences, respectively.\nThe task of sequential pattern mining is an enumeration problem. It aims at enumerating all patterns\n(subsequences) that have a support no less than the minimum support threshold set by the user. Thus,\nthere is always a single correct answer to a sequential pattern mining problem. Discovering sequential\npatterns is a hard problem. To solve this problem, the naive approach is to calculate the support of all\npossible subsequences in a sequence database to then output only those meeting the minimum support\nconstraint speci\ufb01ed by the user. However, such a naive approach is ine\ufb03cient because the number of\nsubsequences can be very large. A sequence containing q items in a sequence database can have up\nto 2q \u2212 1 distinct subsequences. Because of this, applying the naive approach to solve the sequential\n58\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\npattern mining problem is unrealistic for most real-life sequence databases. Hence, it is necessary to\ndesign e\ufb03cient algorithms to avoid exploring the search space of all possible subsequences.\nNumerous algorithms have been designed to discover sequential patterns in sequence databases. Some\nof the most popular are GSP [98], Spade [117], Pre\ufb01xSpan [89], Spam [3], Lapin [111], CM-Spam [30], and\nCM-Spade [30]. All these sequential pattern mining algorithms take as input a sequence database and a\nminimum support threshold (chosen by the user), and output the set of frequent sequential patterns. It is\nimportant to note that there is always only one correct answer to a sequential pattern mining task (for a\ngiven sequence database and threshold value). Thus, all sequential pattern mining algorithms return the\nsame set of sequential patterns if they are run with the same parameter on the same database. Hence,\nthe di\ufb00erence between the various algorithms is not their output, but it is how each algorithm discovers\nthe sequential patterns. Various algorithms utilize di\ufb00erent strategies and data structures to search for\nsequential patterns e\ufb03ciently. As a result, some algorithms are more e\ufb03cient than others.\nIn the following pages, we give an overview of the main techniques employed by sequential pattern\nmining algorithms, and discusses their advantages and limitations. The following section then discusses\nvariations of the sequential pattern mining problem.\nBefore introducing the speci\ufb01c techniques, it is important to present a few key de\ufb01nitions related to\nthe search for sequential patterns. In general, sequential pattern mining algorithms assume that there\nexists a total order on items denoted as \u227b, which represent the order for processing items in a database to\n\ufb01nd the sequential patterns. For example, consider a database containing the items {a, b, c}. The order\nfor processing items could be de\ufb01ned as the lexicographical order (e.g. c \u227b b \u227b a). But any other total\norder on items from I such as the order of decreasing or increasing support could be used. Note that\nthe choice of the order \u227b has no in\ufb02uence on the \ufb01nal result produced by a sequential pattern mining\nalgorithm. The order \u227b is used so that algorithms follow a speci\ufb01c order to explore potential sequential\npatterns, and thus avoid considering the same pattern more than once.\nAll sequential pattern mining algorithms explore the search space of sequential patterns by performing\ntwo basic operations called s-extensions and i-extensions.\nThese operations are used to generate a\n(k + 1)-sequence (a sequence containing k + 1 items) from a k-sequence. These operations are de\ufb01ned\nas follows. A sequence sa = \u27e8A1, A2, ..., An\u27e9 is a pre\ufb01x of a sequence sb = \u27e8B1, B2, ..., Bm\u27e9, if n < m,\nA1 = B1, A2 = B2, ..., An\u22121 = Bn\u22121 and An is equal to the \ufb01rst |An| items of Bn according to the\n\u227b order. For example, the sequence \u27e8{a}\u27e9 is a pre\ufb01x of the sequence \u27e8{a, b}, {c}\u27e9, and the sequence\n\u27e8{a}{c}\u27e9 is a pre\ufb01x of the sequence \u27e8{a}, {c, d}\u27e9 A sequence sb is said to be an s-extension of a sequence\nsa = \u27e8I1, I2, ...Ih\u27e9 with an item x, if sb = \u27e8I1, I2, ...Ih, {x}\u27e9, i.e. sa is a pre\ufb01x of sb and the item x appears\nin an itemset occuring after all the itemsets of sa. For example, the sequence \u27e8{a}, {a}\u27e9 and \u27e8{a}, {b}\u27e9\nand \u27e8{a}, {c}\u27e9 are s-extensions of the sequence \u27e8{a}\u27e9. A sequence sc is said to be an i-extension of sa\nwith an item x, if sc = \u27e8I1, I2, ...Ih \u222a {x}\u27e9, i.e. sa is a pre\ufb01x of sc and the item x is appended to the last\nitemset of sa, and the item x is the last one in Ih according to the \u227b order. For example, the sequences\n\u27e8{a, b}\u27e9 and \u27e8{a, c}\u27e9 are i-extensions of the sequence \u27e8{a}\u27e9.\nIn general, sequential pattern mining algorithms can be categorized as being either depth-\ufb01rst search\nor breadth-\ufb01rst search algorithms.\nBreadth-\ufb01rst search algorithms such as GSP proceed as follows.\nThey \ufb01rst scan the database to \ufb01nd frequent 1-sequences (sequential patterns containing a single item).\nThen, they generate 2-sequences by performing s-extensions and i-extensions of 1-sequences, then 3-\nsequences are generated using the 2-sequences, then 4-sequences are generated using the 3-sequences,\nand so on until no sequences can be generated. This approach is also called a level-wise approach since\npatterns are considered in ascending order of their length. For example, consider a sequence database\ncontaining three items I = {a, b, c}. A breadth-\ufb01rst algorithm will \ufb01rst consider the 1-sequences \u27e8{a}\u27e9,\n\u27e8{b}\u27e9, and \u27e8{c}\u27e9. Then, the algorithm will consider the 2-sequences \u27e8{a}, {a}\u27e9, \u27e8{a}, {b}\u27e9, \u27e8{a}, {c}\u27e9,\n\u27e8{a, a}\u27e9, \u27e8{a, b}\u27e9, \u27e8{a, c}\u27e9, \u27e8{b}, {a}\u27e9, \u27e8{b}, {b}\u27e9, \u27e8{b}, {c}\u27e9, \u27e8{b, a}\u27e9, \u27e8{b, b}\u27e9, \u27e8{b, c}\u27e9, \u27e8{c}, {a}\u27e9, \u27e8{c}, {b}\u27e9,\n\u27e8{c}, {c}\u27e9, \u27e8{c, a}\u27e9, \u27e8{c, b}\u27e9, and \u27e8{c, c}\u27e9. Then, the algorithm will consider the 3-sequences, 4-sequences,\nand so on until no patterns can be generated. It can be observed that the search space can be very large,\nas there are many di\ufb00erent ways to combine items to generate a potential sequential pattern. Assume\nthat the longest sequence in a database contains x items. A breadth-\ufb01rst search sequential pattern mining\nalgorithm will explore in the worst case all possible sequences containing x items or less. If a database\ncontains m items, this number can be greater than 2m.\nDepth-\ufb01rst search algorithms such as Spade [117], Pre\ufb01xSpan [89], Spam [3], Lapin [111], CM-Spam [30],\nand CM-Spade [30] explore the search space of patterns by following a di\ufb00erent order.\nThey start\nfrom the sequences containing single items (e.g.\n\u27e8{a}\u27e9, \u27e8{b}\u27e9, and \u27e8{c}\u27e9), and then recursively per-\nform i-extensions and s-extensions with one of these sequences to generate larger sequences.\nThen,\nwhen a pattern can no longer be extended, the algorithm backtrack to generate other patterns using\nA Survey of Sequential Pattern Mining\n59\nother sequences.\nFor example, consider a sequence database containing the items I = {a, b, c}, and\nconsider that only sequential patterns containing no more than three items are considered (for the pur-\npose of having a small example).\nA depth-\ufb01rst search algorithm assuming the lexicographical order\n(e.g. c \u227b b \u227b a), which performs i-extensions before s-extensions will explore potential sequential pat-\nterns following this order: \u27e8\u27e9, \u27e8{a}\u27e9, \u27e8{a, b}\u27e9, \u27e8{a, b, c}\u27e9, \u27e8{a, c}\u27e9, \u27e8{a}, {a}\u27e9, \u27e8{a}, {a, b}\u27e9, \u27e8{a}, {a, c}\u27e9,\n\u27e8{a}, {b}\u27e9, \u27e8{a}, {b, c}\u27e9, \u27e8{a}, {b}, {c}\u27e9, \u27e8{a}, {c}\u27e9, \u27e8{a}, {c}, {a}\u27e9, \u27e8{a}, {c}, {b}\u27e9, \u27e8{a}, {c}, {c}\u27e9, \u27e8{b}\u27e9,\n\u27e8{b, c}\u27e9, \u27e8{b}, {a}\u27e9, \u27e8{b}, {a, b}\u27e9, \u27e8{b}, {a, c}\u27e9, \u27e8{b}, {a}, {a}\u27e9, \u27e8{b}, {a}, {b}\u27e9, \u27e8{b}, {a}, {c}\u27e9, \u27e8{b}, {b}\u27e9,\n\u27e8{b}, {b, c}\u27e9, \u27e8{b}, {b}, {a}\u27e9, \u27e8{b}, {b}, {b}\u27e9, \u27e8{b}, {b}, {c}\u27e9, \u27e8{b}, {c}\u27e9, \u27e8{b}, {c}, {a}\u27e9, \u27e8{b}, {c}, {b}\u27e9,\n\u27e8{b}, {c}, {c}\u27e9, \u27e8{c}\u27e9, \u27e8{c}, {a}\u27e9, \u27e8{c}, {a, b}\u27e9, \u27e8{c}, {a, c}\u27e9, \u27e8{c}, {a}, {a}\u27e9, \u27e8{c}, {a}, {b}\u27e9, \u27e8{c}, {a}, {c}\u27e9,\n\u27e8{c}, {b}\u27e9, \u27e8{c}, {b, c}\u27e9, \u27e8{c}, {b}, {a}\u27e9, \u27e8{c}, {b}, {b}\u27e9, \u27e8{c}, {b}, {c}\u27e9, \u27e8{c}, {c}\u27e9, \u27e8{c}, {c}, {a}\u27e9,\n\u27e8{c}, {c}, {b}\u27e9, and \u27e8{c}, {c}, {c}\u27e9.\nSince the search space of all possible sub-sequences in a database can be very large, designing an e\ufb03cient\nalgorithm for sequential pattern mining requires to integrate techniques to avoid exploring the whole\nsearch space. The basic mechanism for pruning the search space in sequential pattern mining is based\non the following property called the Apriori property, downward-closure property or anti-monotonicity\nproperty. This property states that for any sequence sa and sb, if sa is a subsequence of sb (sa \u228f sb),\nthen sb must have a support that is lower or equal to the support of sa. For example, consider two\nsequences \u27e8{a}\u27e9 and \u27e8{a, b}\u27e9. It is obvious that the support (the occurrence frequency) of \u27e8{a, b}\u27e9 cannot\nbe greater than the support of \u27e8{a}\u27e9 since \u27e8{a, b}\u27e9 is more speci\ufb01c than \u27e8{a}\u27e9. It is thus said that the\nsupport measure is monotonic.\nThe above property is useful for pruning the search space since if a\nsequence is infrequent, if follows that all its extensions are also infrequent, and thus are not sequential\npatterns. For example, consider the database of Table 1 and assume that minsup = 2. Since the pattern\n\u27e8{c}, {g}\u27e9 is infrequent, all its extensions such as \u27e8{c}, {g}, {e}\u27e9 are also infrequent and hence can be\nignored. The application of the downward-closure property can thus greatly reduce the search space of\nsequential patterns.\nIn general, sequential pattern mining algorithms di\ufb00er in (1) whether they use a depth-\ufb01rst or breadth-\n\ufb01rst search, (2) the type of database representation that they use internally or externally, (3) how they\ngenerate or determine the next patterns to be explored in the search space, and (4) how they count the\nsupport of patterns to determine if they satisfy the minimum support constraint.\nAprioriAll is the \ufb01rst sequential pattern mining algorithm [2]. The authors of AprioriAll then proposed\nan improved version called GSP [98]. The AprioriAll and GSP algorithms are inspired by the Apriori\nalgorithm for frequent itemset mining [1].\nGSP uses a standard database representation, as shown in Table 1, also called a horizontal database.\nThe GSP algorithm performs a level-wise search to discover frequent sequential patterns. It \ufb01rst scans the\ndatabase to calculate the support of all 1-sequences. Then, it keeps all frequent 1-sequences in memory.\nFor example, consider the sequence database of Table 1. The frequent 1-sequences are \u27e8{a}\u27e9, \u27e8{b}\u27e9, \u27e8{c}\u27e9,\n\u27e8{e}\u27e9, \u27e8{f}\u27e9, and \u27e8{g}\u27e9. Then, the GSP algorithm recursively explores larger patterns. During this search,\nGSP uses the sequential patterns of length k to generates potentially frequent patterns of length k + 1.\nThis process of generating candidates is done by combining pairs of patterns of length k that share all\nbut one item. For example, in the above example, GSP will combine the 1-sequential patterns to obtain\n2-sequences: \u27e8{a, b}\u27e9, \u27e8{a, c}\u27e9, \u27e8{a, e}\u27e9, \u27e8{a, f}\u27e9, \u27e8{a, g}\u27e9, \u27e8{a}, {a}\u27e9, \u27e8{a}, {b}\u27e9, \u27e8{a}, {c}\u27e9, \u27e8{a}, {e}\u27e9,\n\u27e8{a}, {f}\u27e9, \u27e8{a}, {g}\u27e9, \u27e8{b, c}\u27e9, \u27e8{b, d}\u27e9, . . . \u27e8{g}, {g}\u27e9. After generating candidates, the GSP algorithm\nwill evaluate each candidate to determine if it is a sequential pattern (if it has a support no less than the\nminsup threshold), to identify patterns that should be output. This is done in two steps. First, for a\ncandidate pattern sa, GSP will check if all its sub-sequences of length k-1 are also frequent. If sa has a\nsubsequence that is not frequent, sa cannot be frequent (it would violate the downward-closure property).\nOtherwise, GSP will scan the database to calculate the support of sa. If sa is frequent, it will be output.\nIn this example, GSP \ufb01nds that the frequent 2-sequences are: \u27e8{a, b}\u27e9, \u27e8{a}, {b}\u27e9, \u27e8{a}, {c}\u27e9, \u27e8{a}, {e}\u27e9,\n\u27e8{a}, {f}\u27e9, \u27e8{a}, {g}\u27e9, \u27e8{b}, {e}\u27e9, \u27e8{b}, {f}\u27e9, \u27e8{b}, {g}\u27e9, \u27e8{c}, {e}\u27e9, \u27e8{c}, {f}\u27e9, \u27e8{f, g}\u27e9, \u27e8{f}, {e}\u27e9, and\n\u27e8{g}, {e}\u27e9. Then the GSP algorithm continues this process to generate sequential patterns of length 3,\n4, and so on, until no pattern can be generated. The \ufb01nal set of sequential patterns is shown in Table 2.\nThe GSP algorithm is well-known since it is one of the \ufb01rst sequential pattern mining algorithms. In\nrecent years, many algorithms have been shown to be more e\ufb03cient than GSP. The reason is that GSP\nhas several important limitations:\n\u2022 Multiple database scans. One of the main problems of GSP is that it repeatedly scans the\ndatabase to calculate the support of candidate patterns. This can be very costly for large database,\neven if some optimizations may be performed to reduce that cost (e.g. by sorting sequences by\ntheir size to avoid comparing long patterns with short sequences).\n60\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\n\u2022 Non-existent candidates. Another problem of GSP is that it may generate patterns that do\nnot exist in the database.\nThe reason is that GSP generates candidates by combining smaller\npatterns without accessing the database. Hence, GSP can waste time considering many patterns\nthat are non-existent in the database. For example, the pattern \u27e8{g}, {g}\u27e9 would be considered as\na potential sequential pattern by GSP for the database of Table 1, although it does not appear in\nthis database.\n\u2022 Maintaining candidates in memory. Another serious problem of the GSP algorithm is typical\nof breadth-\ufb01rst search pattern mining algorithms. It is that at any moment it must keep all frequent\nsequences of length k in memory to be able to generate patterns of length k + 1. This can consume\na huge amount of memory.\nThe Spade [117] algorithm is an alternative algorithm that utilizes a depth-\ufb01rst search, and avoid some\nof the drawbacks of the GSP algorithm. The Spade algorithm is inspired by the Eclat [116] algorithm for\nfrequent itemset mining. It utilizes a vertical database representation rather than an horizontal database\nrepresentation. The vertical representation of a sequence database indicates the itemsets where each item\ni appears in the sequence database [117, 3, 30]. For a given item, this information is called the IDList of\nthe item. For example, Fig. 2 shows the vertical representation of the horizontal database of Figure 1.\nIn this example, the IDList of item g indicates that g occurs in the third and fourth itemsets of sequence\n1, and in the second itemset of sequence 4. The vertical representation of an horizontal database (the\nIDLists of all single items) can be created by scanning the horizontal database once. Note that it is also\npossible to perform the reverse process to create an horizontal database from a vertical database (the\ndi\ufb00erence between an horizontal and vertical reprensentation of a database is just how the information\nis stored).\nFormally, the IDList of an item or pattern is de\ufb01ned as follows.\nLet there be a pattern sa =\n\u27e8A1, A2, ..., An\u27e9 appearing in a sequence sb = \u27e8B1, B2, ..., Bm\u27e9 (sa \u2291 sb).\nThe item-positions of sa\nin sb, denoted as ip(sa, sb), is the set of pairs of the form (sa, ik), where ik is an integer such that\n1 \u2264 i1 < i2 < ... < ik \u2264 m and A1 \u2286 Bi1, A2 \u2286 Bi2, ..., An \u2286 Bik. For example, the item-positions\nof the pattern \u27e8{a}\u27e9 in the sequence s2 of Table 1 is {(s2, 1), (s2, 4)}, indicating that the pattern \u27e8{a}\u27e9\nappears in the \ufb01rst and fourth itemsts of sequence s2. The IDList of a pattern sa for a sequence data-\nbase SDB is de\ufb01ned as the list of all item-positions of sa in all sequences where it appears, that is\nIDList(sa) = S\nsa\u2291sb\u2227sb\u2208SDB ip(sa, sb). For example, the IDList of \u27e8{a}\u27e9 (also depicted in Fig. 2) is\nIDList(sa) = {(s1, 1), (s2, 1), (s2, 4), (s3, 1)}.\nA vertical database has two interesting properties for sequential pattern mining. The \ufb01rst property is\nthat the IDList of any pattern allows to directly calculate its support. The support of a pattern sa is\nsimply the number of distinct sequence identi\ufb01ers in its IDList. For example, the IDList of the pattern\n\u27e8{a, b}\u27e9 is {(s1, 1), (s2, 3), (s2, 4), (s3, 2)}. The number of distinct sequence identi\ufb01ers in this IDList is\n3. Thus, the support of this patterns is 3. The second property is that the IDList of any pattern sa\nobtained by performing an i-extension or s-extension of a pattern sb with an item i can be created without\nscanning the original database by joining the IDList of sb with the IDList of the item i. For example,\nby comparing the IDLists of patterns \u27e8{a}\u27e9 and \u27e8{b}\u27e9, it is possible to obtain the IDList of the pattern\n\u27e8{a, b}\u27e9. The detailed process of this join operation is not presented here. The interested reader may\nrefer to the article describing the Spade [117] algorithm for more details.\nFigure 2. A vertical database\nBy utilizing the above properties, algorithms such as Spade [117], Spam[3], CM-Spam [30], and CM-\nSpade [30] explore the whole search space of patterns by reading the database only once to create the\n3\nA Survey of Sequential Pattern Mining\n61\nIDLists of single items. Then, the IDLists of any pattern encountered when browsing the search space\nis obtained by performing the join of IDLists, which allows to calculate the support of the pattern.\nThus, all frequent patterns can be enumerated without repeatedly scanning the database, and without\nmaintaining a large number of patterns in memory (contrarily to breadth-\ufb01rst search algorithms). As\na result, this approach was shown to be one of the most e\ufb03cient for sequential pattern mining, and to\ngreatly outperform the \ufb01rst algorithms, who adopted a breadth-\ufb01rst search approach.\nThe IDList representation is used in several sequential pattern mining algorithms. A popular opti-\nmization of the IDList structure used in the Spam [3] and BitSpade [4] algorithms is to encode IDLists\nas bit vectors. The motivation for this optimization is that IDLists can be very large when patterns\nappear in many sequences (especially, in dense databases or databases containing long sequences), and\nthat applying the join operation of IDLists is costly as it requires to compare the elements of two IDLists.\nThe solution introduced in the Spam [3] algorithm is to represent IDLists as bit vectors. The bit vector\nrepresentation of an IDList is de\ufb01ned as follows. Let SDB be a sequence database containing k items\nand m sequences, where size(i) denotes the number of itemsets in the i-th sequence of SDB. Consider a\npattern sa and its IDList IDList(sa). The bit vector representation of this IDList, denoted as BList(sa)\nis a bitvector containing Pm\ni=1 size(i) bits, where the j-th bit represents the p-th itemset of the t-th se-\nquence of SDB, such that Pmin(0,t\u22121)\ni=1\nsize(i) < j < Pt\ni=1 size(i) and p = j \u2212 Pmin(0,t\u22121)\ni=1\n. The j-th bit\nis set to 1 if (st, p) \u2208 IDList(sa), and otherwise it is set to 0. For example, the bit vector representation\nof the IDLists of single items is shown in Table 3.\nTable 3. The bitvector representation of the vertical database\nItem x\nIDList of x as a bit vector\na\n100001001100000\nb\n100000011010010\nc\n010000100000000\nd\n000001000000000\ne\n000010001000100\nf\n001000001001001\ng\n001100000000001\nUsing bitvectors to represent IDLists can greatly reduce the amount of memory used for mining\nsequential patterns with a vertical representation. This is especially true for dense datasets where many\nbits are generally set to 1 in IDLists. For sparse datasets, many bits are set to 0, and thus the amount of\nsaved memory is less. Note that it is possible to use various bit vector compression techniques to further\ncompress the bit vectors. For example, some bit vector compression techniques have been used in various\npattern mining algorithm implementations to ignore some bits set to zero [35]. The version of the Spade\nalgorithm using bit vectors is called BitSpade [4]. Another algorithm that is similar to BitSpade and also\nperforms a depth-\ufb01rst search using bit vector IDLists is Spam [3]. Moreover, an algorithm inspired by\nSpam called Fast [100] introduced the concept of indexed sparse IDLists, to more quickly calculate the\nsupport of candidates and reduce memory usage. A version of Spade using bit vectors called Prism also\nintroduced the concept of Prime-block encoding [47]. It was shown that the Spam, BitSpade, and Prism,\nwhich uses the bit vector representation, are more than an order of magnitude faster than the original\nSpade algorithm [4, 3], while the Fast algorithm was shown to be faster than Spam [100] but was not\ncompared with Spade or the improved BitSpade algorithm.\nRecently, the Spam [3] and BitSpade [117] algorithms were improved to obtain the CM-Spam and CM-\nSpade algorithms [30]. These algorithms are based on the observations that Spam and Spade generate\nmany candidate patterns and that performing the join operation to create the IDList of each of them is\ncostly. To reduce the number of join operations, the CM-Spam and CM-Spade algorithm introduced the\nconcept of co-occurrence pruning [30]. It consists of initially scanning the database to create a structure\ncalled the Co-occurrence Map (CMAP) that stores all frequent 2-sequences. Then, for each pattern sa\nthat is considered by the search procedure if the two last items of sa are not a frequent 2-sequences,\nthe pattern sa can be directly eliminated without constructing its IDList (thus without performing the\njoin operation). It was shown that CM-Spam and CM-Spade outperformed GSP, Spam, BitSpade [117],\nand Pre\ufb01xSpan by more than an order of magnitude. CM-Spade is claimed to be the current fastest\nalgorithm [30].\nBesides breadth-\ufb01rst search algorithms and vertical algorithms, another important type of algorithms\nfor sequential pattern mining are the pattern-growth algorithms. These algorithms are depth-\ufb01rst search\n1001\n62\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\nalgorithms, designed to address a limitation of the previously described algorithms, which is to generate\ncandidate patterns that may not appear in the database.\nThe reason why the previously described\nalgorithms may generate patterns not appearing in the database is that they generate candidate patterns\nby combining smaller patterns but this process does not involve accessing the database.\nPattern-growth algorithms avoid this problem by recursively scanning the database to \ufb01nd larger\npatterns. Thus, they only consider the patterns actually appearing in the database. Performing database\nscans can however be costly.\nTo reduce the cost of database scans, pattern-growth algorithms have\nintroduced the concept of projected database [53, 86, 89], which aims at reducing the size of databases as\nlarger patterns are considered by the depth-\ufb01rst search.\nThe most popular pattern-growth algorithm for sequential pattern mining is Pre\ufb01xSpan [89], which\ndraws inspiration from the FPGrowth algorithm for itemset mining [53]. Pre\ufb01xSpan proceeds as follows.\nIt explores the search space of sequential patterns using a depth-\ufb01rst search. It starts from sequential\npatterns containing a single item and explores larger patterns by recursively appending items to patterns\nto create larger patterns. To ensure that no patterns are generated twice, the items are appended to\npatterns according to a total order on items \u227a, which can be the lexicographical order or any other total\norder. The \ufb01rst operation performed by Pre\ufb01xSpan is to scan the original sequence database to calculate\nthe support of single items an identify the frequent items (those having a support that is no less than\nthe minsup threshold). Then, Pre\ufb01xSpan outputs each of these items as a frequent sequential patterns,\nand consider these patterns as seeds to pursue the depth-\ufb01rst search. During the depth-\ufb01rst search, for\na given sequential pattern sa of length k, Pre\ufb01xSpan \ufb01rst creates the projected database of the pattern\nsa. Then, Pre\ufb01xSpan scans the projected database of sa to count the support of items to \ufb01nd items that\ncan be appended to sa by i-extension or s-extension to form (k + 1)-sequential patterns. This process is\nthen recursively repeated as a depth-\ufb01rst search to \ufb01nd all frequent sequential patterns.\nTable 4. The projected sequence database of pattern \u27e8{a}\u27e9\nSID\nSequence\n1\n\u27e8{ , b}, {c}, {f, g}, {g}, {e}\u27e9\n2\n\u27e8{ , d}, {c}, {b}, {a, b, e, f}\u27e9\n3\n\u27e8{b}, {f}, {e}\u27e9\n4\n\u27e8{b}, {f, g}\u27e9\nTable 5. The projected sequence database of pattern \u27e8{a, b}\u27e9\nSID\nSequence\n1\n\u27e8{c}, {f, g}, {g}, {e}\u27e9\n2\n\u27e8{ , e, f}\u27e9\nWe illustrate these steps with a short example. Consider that minsup = 2. By scanning the database of\nFig. 1, Pre\ufb01xSpan will \ufb01nd that the frequent 1 sequences are \u27e8{a}\u27e9, \u27e8{b}\u27e9, \u27e8{c}\u27e9, \u27e8{e}\u27e9, \u27e8{f}\u27e9, and \u27e8{g}\u27e9.\nThese sequences will thus be output. Then, assuming the lexicographical ordering of items, Pre\ufb01xSpan\nwill \ufb01rst consider the item a to try to \ufb01nd larger frequent sequences starting with the pre\ufb01x \u27e8{a}\u27e9.\nPre\ufb01xSpan will thus scan the original database to build the projected database of \u27e8{a}\u27e9, shown in Table\n4. The projected database of the pattern \u27e8{a}\u27e9 is the set of sequences where the pattern \u27e8{a}\u27e9 appears,\nbut where all items and itemsets appearing before the \ufb01rst occurrence of \u27e8{a}\u27e9 have been removed [89].\nThen, to \ufb01nd frequent sequential patterns that starts with the pre\ufb01x \u27e8{a}\u27e9 containing one more item,\nthe Pre\ufb01xspan algorithm will read the projected database of \u27e8{a}\u27e9 and count the support of all items\nappearing in that database that could be appended either by i-extension or s-extension to \u27e8{a}\u27e9. For\ninstance, the 2-sequences that are i-extensions or s-extensions of \u27e8{a}\u27e9 are: \u27e8{a, b}\u27e9 : 2, \u27e8{a, d}\u27e9 : 1,\n\u27e8{a}, {a}\u27e9 : 1, \u27e8{a}, {b}\u27e9 : 3, \u27e8{a}, {c}\u27e9 : 2, \u27e8{a}, {e}\u27e9 : 3, \u27e8{a}, {f}\u27e9 : 4, and \u27e8{a}, {g}\u27e9 : 2, where for each\npattern, the number after the colon (:) indicates its support. Then, Pre\ufb01xSpan will output the sequential\npatterns (those having a support greater than or equal to 2), that is: \u27e8{a, b}\u27e9, \u27e8{a}, {b}\u27e9, \u27e8{a}, {c}\u27e9,\n\u27e8{a}, {e}\u27e9, \u27e8{a}, {f}\u27e9, and \u27e8{a}, {g}\u27e9, Then, Pre\ufb01xSpan will continue its depth-\ufb01rst exploration of the\nsearch space by attempting to \ufb01nd sequential patterns starting with the pre\ufb01x \u27e8{a, b}\u27e9. Pre\ufb01xSpan will\nscan the database of the pattern \u27e8{a}\u27e9 to create the projected database of \u27e8{a, b}\u27e9. This database is\ndepicted in Table 5. While creating the projected database of \u27e8{a, b}\u27e9, Pre\ufb01xSpan will count the support\n, g},{e}\nA Survey of Sequential Pattern Mining\n63\nof items that can extend \u27e8{a, b}\u27e9 by i-extension of s-extension. This process will then continue in the\nsame way (by pursuing the depth-\ufb01rst search) until all sequential patterns have been found.\nThe pattern-growth approach of Pre\ufb01xSpan has the advantage that it only explores patterns appearing\nin the database (unlike many other sequential pattern mining algorithms).\nHowever, a drawback of\nPre\ufb01xSpan and other pattern-growth algorithms is that it can be costly to repeatedly scan the database\nand create database projections, in terms of runtime. Moreover, in terms of memory, creating database\nprojections can consume a huge amount of memory if it is naively implemented, as in the worst case it\nrequires to copy almost the whole database for each database projection. In the Pre\ufb01xSpan algorithm\nan optimization called pseudo-projection is used to reduce this cost, which consists of implementing a\nprojected database as a set of pointers on the original database [89, 86]. Another notable pattern-growth\nsequential pattern mining algorithms is FreeSpan [51], an early version of Pre\ufb01xSpan, proposed by the\nsame research team.\nThe time complexity of sequential pattern mining algorithms depends on the number of patterns in the\nsearch space, and the cost of the operations for generating and processing each itemset. Pattern-growth\nalgorithms have the advantage over other algorithms of only considering patterns that actually appear in\nthe database. Thus, it would seem reasonable to expect that they would be faster than other algorithms.\nHowever, in practice, it has been reported that this is not the case. The CM-Spade algorithm was for\nexample reported to outperform Pre\ufb01xSpan by more than an order of magnitude [30]. The reason is that\nthe cost of scanning the database and performing projections can be quite high.\nThe number of patterns in the search space depends on how the minsup threshold is set by the user\nand on how similar the sequences are in a sequence database. In general, as the minsup threshold is\ndecreased the number of sequential patterns found by sequential pattern mining algorithms can increase\nexponentially. The search space for sequential pattern mining can be very large even for small sequence\ndatabases containing a few sequences. For example, a sequence database containing only two identical\nsequences of 100 items can contain 2100 sequential patterns.\nIn the above paragraphs, we have discussed the three main types of sequential pattern mining algo-\nrithms: breadth-\ufb01rst algorithms that perform candidate generation (e.g. AprioriAll, and GSP), depth-\n\ufb01rst search algorithms that perform candidate generation using the IDList structure and its variations (e.g.\nSpade, Spam, BitSpade, Fast, CM-Spam, CM-Spade), and pattern-growth algorithms (e.g. FreeSpan,\nPre\ufb01xSpan). Most sequential pattern mining algorithms extends these three main approaches.\n3. Variations of the Sequential Pattern Mining Problem. The task of sequential pattern mining\nhas many applications. Nonetheless, for some applications, it also has some fundamental limitations.\nTo address this issue, several extensions or variations of the problem of sequential pattern mining have\nbeen proposed.\nThis section aims at providing and up-to-date review of some of the most popular\nvariations/extensions.\nA \ufb01rst important limitation of the traditional problem of sequential pattern mining is that a huge\nnumber of patterns may be found by the algorithms, depending on a database\u2019s characteristics and how\nthe minsup threshold is set by users. Finding too many patterns is an issue because users typically do\nnot have much time to analyze a large amount of patterns. Moreover, as more patterns are found, the\nalgorithms\u2019 performance typically decrease in terms of memory and runtime. To address this issue, a\nsolution that has been extensively studied is to discover concise representations of sequential patterns\ninstead of all sequential patterns [30, 31, 32, 28, 108, 109, 45, 44, 67, 64, 63, 46, 70, 92, 114, 48, 56]. A\nconcise representation is a subset of all sequential patterns that is meaningful and summarize the whole\nset of sequential patterns.\nSeveral concise representations of sequential patterns have been proposed and several algorithms have\nbeen designed to directly discover these representations without extracting all sequential patterns. It\nwas shown that these algorithms can be order of magnitudes faster than traditional sequential pattern\nmining algorithms and \ufb01nd a much smaller set of patterns. Some of these concise representations were\nalso shown to provide higher classi\ufb01cation accuracy compared to using all sequential patterns for classi-\n\ufb01cation tasks [46, 92]. Let FS denotes the set of all sequential patterns. There are three main concise\nrepresentations of sequential patterns.\n\u2022 Closed sequential patterns [56, 48, 30, 108, 109] are the set of sequential patterns that are not in-\ncluded in other sequential patterns having the same support, that is: CS = {sa|sa \u2208 FS\u2227 \u0338 \u2203sb \u2208 FS\nsuch that sa \u228f sb \u2227 sup(sa) = sup(sb)}. For example, consider the sequence database of Table 1.\nAmong the 29 sequential patterns found in this database for minsup = 2, only 16 of them are\nclosed sequential patterns (identi\ufb01ed in Table 2). Discovering closed sequential patterns instead of\n64\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\nall sequential patterns thus considerably reduce the result set presented to the user. The closed se-\nquential patterns are interesting because they are a lossless representation of all sequential patterns,\nthat is using the closed sequential patterns, it is possible to recover all the sequential patterns and\ntheir support without accessing the database. Another reason why the closed sequential patterns\nare interesting is that they represents the largest subsequences common to sets of sequences. For\nexample, in market basket analysis, if each sequence represents a customer, the closed patterns\nrepresent the largest patterns common to groups of customers.\nThe \ufb01rst algorithms for closed\nsequential pattern mining are CloSpan [109] and Bide [108], which are pattern-growth algorithms\nextending Pre\ufb01xSpan. Recent algorithms such as Clasp [48], CloFast [39] and CM-Clasp [30] adopt\na vertical representation and were shown to outperform early algorithms.\n\u2022 Maximal sequential patterns [31, 28, 45, 44, 67, 64, 63] are the set of sequential patterns that are\nnot included in other sequential patterns, that is: MS = {sa|sa \u2208 FS\u2227 \u0338 \u2203sb \u2208 FS such that\nsa \u228f sb}. For example, Table 2 shows that among all the 29 sequential patterns found in the\ndatabase of Table 1, only 10 are maximal. An interesting property of maximal patterns is that\n(MS \u2286 CS \u2286 FS). In other words, the set of maximal patterns is always not larger than the\nset of closed sequential patterns and all sequential patterns. In practice, the number of maximal\npatterns can be of several orders of magnitude less than closed patterns or all patterns. However,\nmaximal sequential patterns are not lossless. The maximal patterns may be used to obtain all\nsequential patterns without scanning the database, but their support can only be recovered by\nperforming an additional database scan [31]. Several algorithms have been proposed for maximal\nsequential pattern mining [31, 28, 45, 44, 67, 64, 63], including breadth-\ufb01rst search algorithms (e.g.\nAprioriAdjust [63]), pattern-growth algorithms (e.g. MaxSP [28]), and vertical algorithms (e.g.\nVMSP [31]). Moreover, approximate algorithms have also been proposed such as DIMASP [45].\nMaximal sequential patterns have several applications such as to \ufb01nd the frequent longest com-\nmon subsequences to sentences in texts, analysing DNA sequences, data compression and web log\nmining [45].\n\u2022 Generator sequential patterns (aka sequential generators) [32, 46, 70, 92, 114] are the set of sequen-\ntial patterns that have no subsequence having the same support, that is: GS = {sa|sa \u2208 FS\u2227 \u0338 \u2203sb \u2208\nFS such that sb \u228f sa \u2227 sup(sa) = sup(sb)}. For example, 14 sequential patterns are generators in\nthe database of Table 1, for minsup = 2 (depicted in Table 21). The set of sequential generators\nis a subset of all sequential patterns, but can be larger, equal or smaller than the set of closed\npatterns [70]. However, it can be argued that generators are preferable to other representations\naccording to the MDL (Minimum Description Length) principle [9] as generators are the smallest\nsubsequences that characterize group of sequences in a sequence database [32, 70, 92]. Additionally,\ngenerators can be combined with closed patterns to generate rules with a minimum antecedent and\na maximum consequent, which allows deriving the maximum amount of information based on the\nminimum amount of information [32]. This can be useful for example in market basket analysis [70].\nOther usage of generators is for classi\ufb01cation, where they were shown to provide higher accuracy\nthan using all patterns or closed patterns [46, 92]. Pattern-growth algorithms for mining sequen-\ntial generators are GenMiner [70], FEAT [46] and FSGP [114]. A more recent algorithm named\nVGEN [32], which extends the CM-Spam algorithm, was shown to ouperform FEAT and FSGP.\nAlgorithms for discovering concise representations of patterns are generally inspired by similar work on\nitemset mining such as for the discovery of closed itemsets [65, 106, 120, 83, 7, 107], maximal itemsets [106],\nand generator itemsets [101, 33, 103]. Several techniques and theoretical results from these works have\nbeen adapted to sequential pattern mining. However, there are also some key di\ufb00erences between concise\nrepresentation of itemsets and sequential patterns. Some of the key di\ufb00erences are that in sequential\npattern mining, the set of generator patterns is not guaranteed to be a subset of the set of closed\npatterns, and multiple closed patterns may correspond to a same set of sequences [70].\nAlgorithms\nfor mining concise representations generally adopt a candidate-maintenance-and-test approach, which\nconsists of exploring the search space of sequential patterns and keeping candidate patterns that may\nbelong to the desired concise representations in memory, until the \ufb01nal result is obtained (e.g. CloSpan,\nVGEN, VMSP). Other algorithms, which are said to be without candidate generation (e.g. BIDE and\nMaxSP), perform database scans to directly output patterns rather than keeping potential candidates in\nmemory.\n1Note that the empty sequence \u27e8\u27e9 is also considered to be a sequential generator, with a support of 4\nsequences, although it is not shown in Table 2.\nA Survey of Sequential Pattern Mining\n65\nTo reduce the number of sequential patterns found and \ufb01nd more interesting patterns, researchers have\nalso proposed to integrate constraints in sequential pattern mining [91]. A constraint is an additional set\nof criteria that the user provides to indicate more precisely the types of patterns to be found. Numerous\nkinds of constraints have been studied. There are two ways of applying constraints. The \ufb01rst way is to\napply them as a post-processing step on the set of all sequential patterns to \ufb01lter uninteresting patterns.\nHowever, a problem with this approach is that enumerating all sequential patterns can consume a lot of\ntime and requires a huge amount of memory. The second way to address this problem is to push the\nconstraints deep in the mining process. In other words, the constraints are applied during the search\nfor patterns to reduce the search space. Algorithms adopting this approach can be orders of magnitude\nfaster, and generate much less patterns than traditional sequential pattern mining algorithms, depending\non the constraints used.\nOne of the \ufb01rst sequential pattern mining algorithm to integrate constraints is GSP [98]. It intro-\nduced the constraints of minimum and maximum amount of time between two consecutive itemsets in\nsequential patterns (gap constraints), as well as a maximum time duration for each sequential pattern\n(duration constraint). The consideration of time constraints has also been the subject of the Hirate and\nYamana algorithm [113] and Fournier-Viger algorith [22], which have extended the Pre\ufb01xSpan and BIDE\nalgorithm with gap and duration constraints. The integration of gap constraints in vertical algorithms\nwas done in Pex-Spam [54] by extending the Spam algorithm. Pei et al. studied the integration of various\nconstraints in pattern-growth algorithms such as items that should appear or not in sequential patterns\n(item constraints), minimum/maximum number of items per sequential patterns (length constraints),\nand aggregate constraints on prices of items in sequences such as average, minimum, maximum, sum\nand standard deviation of prices (aggregate constraints) [91]. Another type of constraints considered\nin sequential pattern mining is regular expressions (regular expression constraints). The SPIRIT [42]\nalgorithm lets users specify regular expressions on patterns to be found. It converts constraints to an\nautomaton for pruning patterns when performing a breadth-\ufb01rst search.\nPei et al. [91] and other researchers studied the characteristics of constraints that can be pushed deep\ninto the process of mining sequential patterns, and other types of patterns [87, 88, 10]. Three main types of\nconstraints have been identi\ufb01ed. Anti-monotone constraints such as the minimum support threshold and\nlength constraints, gap constraints and duration constraints are some of the easiest and most bene\ufb01cial\nto integrate in a pattern mining algorithm, as they can be used to prune the search space by applying\nthe downward closure property. Convertible constraints are constraints that are neither monotone nor\nanti-monotone but that can be converted to anti-monotone constraints if some additional strategies are\napplied [87]. A succinct constraint is a constraint that can be checked for a pattern by only looking at\nthe single items that it contains. For example, the constraint that the sum of the weights of a sequential\npattern should be not greater or not less than a given value can be checked by simply adding the weights\nof its items. This constraint is both succint and anti-monotone. For more information about the use of\nconstraints, the reader may refer to the referenced papers [91, 87, 88, 10].\nAnother limitation of traditional sequential pattern mining studies is that they solely focus on the\ndiscovery of items that are positively correlated in sequence databases. However, for some applications,\nnegative correlations are more interesting than positive correlations.\nTo address this limitation, the\nproblem of mining negative sequential patterns has been studied [118, 57, 119, 20]. A negative sequential\npattern is a pattern containing the negation of a least one item. Mining negative sequential patterns is\nmore di\ufb03cult than mining only positive patterns as the search space becomes larger. To mine positive\nand/or negative patterns, the Negative-GSP [118] and PNSP [57] algorithms were proposed, extending\nGSP. Then, a genetic algorithm was proposed, which was shown to outperform these two algorithms [119],\nas well as a SPADE-based algorithm named e-NSP [20].\nAnother interesting extension of the problem of sequential pattern mining is multi-dimensional sequen-\ntial pattern mining [22, 85, 99]. It considers an extended type of sequence database where each sequence\ncan be annotated with dimensions having symbolic values. For example, in the context of market basket\nanalysis, a database of customer purchase sequences could be annotated with three dimensions: gender,\neducation level, and income. Then, a multi-dimensional sequential pattern mining algorithm can dis-\ncover sequential patterns that are common to various dimension values. For example, a pattern could\nbe discovered with the dimension values (male, university, \u2217) indicating that it is common to some male\ncustomers having a university degree but with any income. To mine multi-dimensional patterns, there are\ntwo main approaches: mining the dimensions using an itemset mining algorithm and then the sequential\npatterns, or mining the sequential patterns and then mining the dimensions [22, 85, 99].\nAnother limitation of traditional sequential pattern mining algorithms is that they assume that se-\nquence databases are static. In fact, traditional sequential pattern mining algorithms are said to be\n66\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\nbatch algorithms as they are designed to be applied once to a sequence database to obtain patterns.\nThen, if the database is updated, algorithms need to be run again from scratch to obtain the updated\npatterns. This approach is ine\ufb03cient because sometimes only small changes are made to a database that\nwould not require to perform the whole search for patterns again. As a solution to this problem, several\nincremental sequential pattern mining algorithms have been designed [13, 78, 82, 76, 74, 75]. To our\nbest knowledge, the \ufb01rst algorithm is ISM [84], which adopts an IDList approach inspired by SPADE to\nupdate sequential patterns when new sequences are inserted in a sequence database (sequence insertion).\nAnother algorithm is IncSpan [13], which is based on the Pre\ufb01xSpan algorithm. However, this algorithm\nwas found to rely on an incorrect property, and was \ufb01xed by Nguyen et al. [82]. Another recent algorithm\nnamed PreFUSP-TREE-INs [74] is based on the pre-large concept, which consists of keeping a bu\ufb00er of\nalmost frequent sequential patterns in memory to avoid scanning the database when few sequences are\ninserted. Recently, algorithms have been also designed to handle sequence modi\ufb01cations [76] and sequence\ndeletions [75]. Among incremental algorithms, a few of them have also been designed to interactively\nmine patterns by for example considering that the user may change the parameters of the algorithm and\nperform multiple queries involving constraints [84]. Another interactive algorithm is KISP, which extends\nthe GSP algorithm [62].\nA particular type of incremental algorithms called stream mining algorithms have been designed to\nmine sequential patterns in a potentially in\ufb01nite stream of sequences [55, 96, 14, 69].\nThey assume\nthat sequences may arrive at a very high speed and may be only read once.\nThey are approximate\nalgorithms that are designed to process each sequence as quickly as possible. To our best knowledge,\nthe \ufb01rst algorithm for mining sequential patterns in streams is eISeq [14].\nHowever, this algorithm\nconsiders a simple type of sequence database where sequences of items are taken as input rather than\nsequences of itemsets. The IncSPAM [55] algorithm is designed for the general case. It extends the\nSPAM algorithm [55]. Algorithms were also designed for mining concise representations of patterns in\nstreams. For example, SPEED [96] and Seqstream [15] mine maximal and closed sequential patterns in\ndata streams, respectively. Generally, stream mining algorithms consider a recency constraint to discover\nsequential patterns that are recently recent (but may be infrequent in the past).\nAnother extension of sequential pattern mining is top-k sequential pattern mining [29]. It consists of\ndiscovering the k most frequent sequential patterns in a sequence database. The rationale for this problem\nis that it is often di\ufb03cult for user to set the minsup threshold using traditional sequential pattern mining\nalgorithms if the user has no background knowledge about the database. If the minsup threshold is set\ntoo low, too many patterns may be found and the algorithms may become very slow, and if the minsup\nthreshold is set too high, too few patterns may be found. Top-k sequential pattern mining algorithms\naddress this problem by letting the users directly indicate the number of patterns k to be found instead\nof using the minsup parameter. Top-k sequential pattern mining is a harder problem than sequential\npattern mining [29].\nSome other extensions of sequential pattern mining extends the sequence database representation in\nvarious ways to extract more rich patterns. The following paragraphs reviews some of the most popular\nextensions.\n\u2022 Weighted sequential pattern mining is an extension of sequential pattern mining where weights\n(generally assumed to be in the [0,1] interval) are associated to each item to indicate their relative\nimportance [18, 112, 97].\nThe goal of weighted sequential pattern mining is to \ufb01nd sequential\npatterns that have a minimum weight.\n\u2022 High-utility sequential pattern mining (HUSPM) is an extension of weighted sequential pattern\nmining where not only item weights are considered but also item quantities in sequences [5, 115,\n72, 8]. The traditional problem of sequential pattern mining takes as input a sequence database\nwhere purchase quantities are binary, that is each item appears in an itemset of a sequence or not.\nFor several applications, this assumption does not hold. For example, consider a sequence database\nof customer transactions where customers may have bought zero, one, or several units of each\nproduct. Not considering the purchase quantities may lead to discovering misleading patterns. To\naddress this limitation, HUSP generalizes the problem of sequential pattern mining by considering\nthat each item appears zero, once or multiple times in each itemset (purchase quantities), and that\neach item has a weight indicating its relative importance (e.g. how much pro\ufb01t is generated by\neach unit sold of the item). The goal of HUSP is to \ufb01nd all sequential patterns that have a utility\ngreater than or equal to a minimum utility threshold in a sequence database. The utility (pro\ufb01t)\nof a sequential pattern is the sum of the maximum utility (pro\ufb01t) generated by the pattern in each\nsequences where it appears [5, 115, 72, 8]. HUSP is quite challenging as the utility measure is\nneither monotone nor anti-monotone unlike the support measure traditionally used in sequential\nA Survey of Sequential Pattern Mining\n67\npattern mining.\nThus, the utility measure cannot be directly used to prune the search space.\nTo address this issue, HUSP algorithm have introduced upper-bounds on the utility of sequential\npatterns such as the SWU [5] measure that are monotone, to prune the search space. A major\nchallenge in HUSP has been to develop tighter upper-bounds on the utility to be able to prune\na larger part of the search space, and improve the performance of HUSP [5, 115, 72, 8]. HUSP\nis a very active research topic. Various extensions of the HUSP problem have been studied such\nas to hide high utility sequential patterns in databases to protect sensitive information [95] and\ndiscovering high-utility sequential rules [125].\n\u2022 Uncertain sequential pattern mining is an extension of sequential pattern mining that considers\ndata uncertainty [80, 81, 121, 49, 50]. For many applications, data stored in databases is uncer-\ntain for various reasons such that data has been collected using noisy sensors or that the data is\ninaccurate or imperfect. Two main models have been designed to discover sequential patterns that\nfrequently appears in uncertain sequence databases [80]. The \ufb01rst model is called the expected-\nsupport model [80]. According to this model, each item i appearing in an itemset X is annotated\nwith an existence probability pr(i, X) representing the certainty that this item appeared in the\nitemset (a value in the [0,1] interval). The expected-support of a sequential pattern sa in a se-\nquence is de\ufb01ned as the product of the expected-support of its items in the sequence if it appears in\nthe sequence, and otherwise 0. The expected-support of a sequential pattern sa in a database SDB\nis the sum of its expected-support values for all sequences where sa appears2. The task of uncertain\nsequential pattern mining in the expected support model is to discover all sequential patterns that\nare expected to be frequent. A variation of this model also considers source uncertainty (whether a\nsequence can be attributed to a given source instead of other(s)) [80]. The second model is the prob-\nabilistic sequential pattern mining model [81]. To use this model, the user has to set two thresholds:\na minimum con\ufb01dence threshold minprob and a minimum support threshold minsup. A sequential\npattern is then considered a probabilistic sequential pattern if the calculated probability that it\nappears in more than minsup transactions by considering possible worlds is greater than minprob.\nVarious algorithm have been proposed for uncertain sequential pattern mining. Algorithms based\non GSP, SPAM and Pre\ufb01xSpan were presented for the expected support model [81]. A Pre\ufb01xS-\npan based algorithm named SeqU-Pre\ufb01xSpan was proposed for the probabilistic sequential pattern\nmining model [121]. In a variation of the problem of uncertain sequential pattern mining [49], the\nuncertainty of timestamps for events was considered. Besides, designing scalable algorithms for\nuncertain sequential pattern mining using the Spark big data framework has also been studied [50].\n\u2022 Fuzzy sequential pattern mining is another important extension of sequential pattern mining [21, 52].\nIt considers databases where items in sequences takes quantitative values (in the [0,1] interval), and\nwhere fuzzy membership functions are used to map these values to nominal values. For example,\nsome items chocolate bar and soft drink in a sequence could be annotated with values of 0.5 and\n0.7 representing their respective sweetness, which could be translated to nominal values such as\nsweet and very sweet. To compute the support in a fuzzy sequence database, various algorithms\nhave been proposed [21, 52] based on di\ufb00erent ways of counting the occurrence of patterns in fuzzy\nsequences. For instance, the SpeedyFuzzy and MiniFuzzy algorithms count a pattern as appearing\nin a sequence if (1) the membership values of all its items are greater than 0, or (2) if they are\ngreater than some threshold, respectively [21]. Numerous studies about discovering other types of\nfuzzy patterns such as fuzzy itemsets have also been published [16, 73].\n4. Other pattern mining problems. Besides sequential pattern mining, several other pattern mining\nproblems have been studied. Research on these problems have inspired research on sequential pattern\nmining. This sections reviews some of the most important related problems.\n\u2022 Itemset mining is the task of discovering frequent itemsets [1, 53, 116, 86, 106] in a transaction\ndatabase. It can be seen as a special case of the sequential pattern mining problem where each\nsequence contains a single itemset. Thus, all items are assumed to be simultaneous, and there is\nno sequential ordering between items. Formally, a transaction database D is a set of transactions,\nwhere each transaction T is an unordered set of items.\nAn itemset X is a set of items.\nThe\nsupport of an itemset X is the number of transactions that contain the itemset, that is sup(X) =\n|{T|X \u2286 T \u2227 T \u2208 D}|. Given a minimum support threshold minsup set by the user, the goal of\nfrequent itemset mining is to \ufb01nd all frequent itemsets, that is all itemsets having a support no less\n2Note that if a pattern is allowed to appear multiple times in a sequence, this de\ufb01nition needs to be\ngeneralized.\n68\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\nthan minsup. Numerous extensions of the problem of itemset mining are similar to variations of\nthe sequential pattern mining problem such as high-utility itemset mining [71, 36, 126], uncertain\nitemset mining [121, 49, 50], fuzzy itemset mining [16, 73], and stream itemset mining [12, 102].\n\u2022 Association rule mining (ARM) [1, 26, 68] consists of \ufb01nding association rules in a transaction data-\nbase. ARM also does not consider the sequential ordering of items. An association rule is a pattern\nof the form X \u2192 Y where X and Y are two itemsets such that X\u2229Y = \u2205. Association rules are eval-\nuated using interestingness measures. The two traditional measures used in association rule mining\nare the support (sup(X \u2192 Y ) = sup(X\u222aY )) and con\ufb01dence (conf(X \u2192 Y ) = sup(X\u222aY )/sup(X)).\nThe support of a rule measures how often it appears in a database, while the con\ufb01dence can be\nseen as a measure of the conditional probability P(Y |X). To \ufb01nd association rules, a user has to\nprovide two thresholds: a minimum support threshold and a minimum con\ufb01dence threshold. The\nresult of ARM is the set of all rules having a support and con\ufb01dence respectively no less than these\nthresholds. An advantage of association rules over frequent itemsets is that association rules not\nonly assess how frequently items co-occur but also if there is a strong association between them.\nIt is interesting to note that besides the support and con\ufb01dence, more than 20 other interesting-\nness measures have been proposed in the literature. The interested reader may refer to Lenca et\nal. [68] for a survey of association rule measures. Generating association rules is generally done in\ntwo phases: mining frequent itemsets in a transaction database, and then using these itemsets to\ngenerate the rules [1].\n\u2022 Sequential rule mining is a variation of the sequential pattern mining problem where sequential\nrules [38, 34] of the form X \u2192 Y are discovered, indicating that if some items X appear in a\nsequence it will be followed by some other items Y with a given con\ufb01dence. The concept of a\nsequential rule is similar to that of association rules excepts that it is required that X must appear\nbefore Y according to the sequential ordering, and that sequential rules are mined in sequences\nrather than transactions. Sequential rules address an important limitation of sequential pattern\nmining, which is that although some sequential patterns may appear frequently in a sequence\ndatabase, the patterns may have a very low con\ufb01dence and thus be worthless for decision-making\nor prediction. For example, consider the database of Table 1. The sequential pattern \u27e8(f)(e)\u27e9\nis considered frequent if minsup = 2 because this pattern appears in 2 sequences. Thus, it may\nbe tempting to think that f is likely to be followed by e in other sequences. However, this is\nnot the case.\nBy looking at Table 1, it can be found that f is actually followed by e in only\ntwo of the four sequences where f appears. This example shows that sequential patterns can be\nmisleading. Sequential rules addresses this problem by not only considering their support but also\ntheir con\ufb01dence. For example, the sequential rule {f} \u2192 {e} has a support of 2 sequences and a\ncon\ufb01dence of 50%, indicating that although this rule is frequent, it is not a strong rule. Formally,\nthe con\ufb01dence of a sequential rule X \u2192 Y is de\ufb01ned as the number of sequences containing the items\nX before the items Y divided by the number of sequences containing the items X [38]. Numerous\nsequential rule mining have been proposed such as RuleGrowth [38] and ERMiner [34], which\nrespectively adopt a pattern-growth and a vertical approach for discovering rules. Moreover, several\nvariations of the problem have been proposed for example to mine sequential rules with a window\nconstraint [38], mine high-utility sequential rules [125] and to discover the top-k most frequent\nsequential rules [24]. Sequential rules have been reported as more e\ufb00ective than sequential patterns\nfor some tasks involving prediction [38]. Sequential rule mining has numerous applications such as\ne-learning, web page prefetching, anti-pattern detection, alarm sequence analysis and restaurant\nrecommendation [34].\n\u2022 Episode mining is another pattern mining problem that shares many similarities with sequential\npattern mining. The main di\ufb00erence is that episode mining aims at \ufb01nding patterns in a single\nsequence rather than a set of sequences.\nThe goal of episode mining is to \ufb01nd itemsets (sets\nof items) frequently appearing in a sequence (frequent episodes) or to discover rules of the form\nX \u2192 Y (episode rules), indicating that X often appears before Y within a time window set by\nthe user. Epsiode mining can be used to analyze various types of data such as web-click streams,\ntelecommunication data, sensor readings, sequences of events on an assembly line and network\ntra\ufb03c data [77, 122].\n\u2022 Periodic pattern mining [41, 105, 60, 61] consists of \ufb01nding patterns that appear frequently and pe-\nriodically in a single sequence. The periodicity of a pattern is measured based on its period lengths.\nThe period lengths of a pattern are the time elapsed between any two consecutive occurrences of the\npattern. To discover periodic patterns, a user has to specify constraints on period lengths such as a\nminimum and maximum average period length, and a maximum period length [105]. Applications\nA Survey of Sequential Pattern Mining\n69\nof periodic pattern mining are numerous, and include stock market analysis, market analysis and\nbioinformatics [41].\n\u2022 Sub-graph mining [110, 59, 11] is another popular pattern mining problem. The goal of sub-graph\nmining is to \ufb01nd all frequent sub-graphs in a large graph or a database of graphs. To mine sub-\ngraphs, the user must specify a minimum support threshold. Then, a sub-graph mining algorithms\noutputs all graphs appearing frequently. Unlike sequential pattern mining, the traditional problem\nof sub-graph mining do not consider the time dimension. Graph mining is a quite challenging task\nbecause the search space can be very large even for small graph databases and it is necessary to de-\nsign strategies to check if di\ufb00erent generated graphs are isomorphic [110]. Various extensions of the\nproblem of sub-graph mining have been studied such as closed and maximal sub-graph mining [59].\nSub-graph mining has various applications such as the analysis of chemical compounds [110, 59, 11].\n5. Large Research opportunities. The problem of sequential pattern mining and other related prob-\nlems have been studied for more than two decades, and are still very active research areas. There are\nmany possibilities for further research on sequential pattern mining.\nBelow, we list some important\nresearch opportunities.\n\u2022 Applications. An important research opportunity is to utilize sequential pattern mining in new\napplications, or in new ways for existing applications. Since sequential data occurs in many \ufb01elds,\nthere are many ways that sequential pattern mining can be applied. One of the most interesting\nand promising possibility is to apply sequential pattern mining in emerging research \ufb01elds such as\nthe internet of things, social network analysis and sensor networks.\n\u2022 Developing more e\ufb03cient algorithms. Sequential pattern mining is computationally expensive in\nterms of runtime and memory. This can a problem for dense databases, or databases containing\nnumerous sequences or long sequences, depending on the minsup threshold chosen by the user. For\nthis reason, many studies have been done on improving the performance of sequential pattern min-\ning algorithms by developing novel data structures and algorithms, and introducing constraints.\nAlthough current algorithms are much faster than the \ufb01rst algorithms, there is still a need for\nimprovement. Besides, more research should be carried on designing e\ufb03cient algorithms for vari-\nations of the sequential pattern mining problem such as uncertain sequential pattern mining and\nhigh-utility sequential pattern mining. Some promising areas of research are the design of parallel,\ndistributed, multi-core, and GPU-based algorithms. In general, depth-\ufb01rst search algorithms are\neasier to parallelize than breadth-\ufb01rst search algorithms.\n\u2022 Designing algorithms to handle more complex data. Another interesting research opportunity is to\nextend sequential pattern mining algorithms so that they can be applied to sequence databases con-\ntaining more complex types of data. Some extensions have been mentioned in this paper. However,\nthere are still many possibilities. This research is important since it will allows sequential pattern\nmining to solve new real-world problems. Some recent researches have for example considered the\nspatial dimension [19].\n\u2022 Designing algorithms for \ufb01nding more complex and meaningful patterns. Another imported issue\nis to \ufb01nd more complex patterns in sequences. Furthermore, to \ufb01nd more meaningful patterns,\nresearch should be further carried to develop interestingness measures and evaluation methods for\nevaluating the usefulness of the patterns found [68]. This would lead to \ufb01nding more interesting\nand useful patterns.\n6. Open-source Implementations. Although sequential pattern mining has been studied for more\nthan two decades, an important issue is that the majority of researchers in this \ufb01eld do not release their\nimplementations or source code. This is problematic because other researchers often need to re-implement\nalgorithms from other researchers to be able to use their algorithms or compare their performance with\nnovel algorithms. But when a researcher implement the algorithm of another researcher, there always\nremains a doubt that the implementation may not be as e\ufb03cient as the original algorithm. Besides,\neven when binary \ufb01les are released it has been noted in studies such as the one of Goethal [43] that\nthe performance of pattern mining algorithms may vary greatly depending on the compiler used and the\nmachine architecture used for running performance comparison.\nThe solution to the above issue is that more researchers share implementations of their algorithms. To\nour best knowledge, the largest collection of open-source implementations of sequential pattern mining\nalgorithms is the SPMF data mining library [35, 40] ( http://www.philippe-fournier-viger.com/\nspmf/). It provides more than 120 algorithms for discovering patterns in databases such as sequential\n70\nP. Fournier-Viger, Jerry C. W. Lin, R. U. Kiran, Y. S. Koh and R. Thomas\npatterns, sequential rules, periodic patterns, itemsets and association rules. The SPMF library is devel-\noped in Java and is multi-platform. Its source code is released under the GPL3 license. It is designed\nto be easily integrated in other Java software programs, and can be run as a standalone software using\nits command-line or graphical user interface. Datasets used for benchmarking sequential pattern mining\nalgorithms can be found on the SPMF website.\nAnother important issue related to the public release of algorithm implementations is that many\nresearchers do not compare the performance of new algorithms with the previous best algorithms. It is\nrecommended that researchers proposing new algorithms compare their performance with the previous\nbest algorithms.\n7. Conclusion. This paper has provided a detailled survey of sequential pattern mining. The paper\nhas presented the main types of algorithms for discovering sequential patterns. Moreover, the paper has\npresented important extensions of the sequential pattern mining problems that address some shortcom-\nings of sequential pattern mining. In addition, the paper has discussed other research problem related\nto sequential pattern mining such as itemset mining, association rule mining, sequential rule mining\nand periodic pattern mining. Finally, the paper has discussed research opportunities and open-source\nimplementations of sequential pattern mining algorithms.\nAcknowledgment. This work is supported by the Youth 1000 Talent funding from the National Science\nFundation of China, and an initiating fund from the Harbin Institute of Technology.\n"
}